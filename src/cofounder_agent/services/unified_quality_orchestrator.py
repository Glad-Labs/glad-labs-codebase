"""
Unified Content Quality Orchestrator

Orchestrates the complete quality evaluation workflow:
1. Content generation
2. Pattern-based quality evaluation (fast)
3. QA Agent review (LLM-based)
4. Hybrid scoring (combines both)
5. Persistence and feedback

This enables:
- Multi-perspective evaluation (pattern + expert)
- Automatic content refinement on low scores
- Comprehensive audit trail
- Scalable quality gating
"""

import logging
from datetime import datetime
from typing import Any, Dict, Optional, Tuple

from fastapi import Depends

logger = logging.getLogger(__name__)


class UnifiedQualityOrchestrator:
    """
    Coordinates complete quality evaluation workflow.

    Flow:
        1. Content generated by content agent
        2. Pattern-based evaluation (async, parallel)
        3. QA Agent evaluation (LLM-based)
        4. Hybrid scoring (combines both)
        5. Store results
        6. Return feedback and pass/fail

    Usage:
        orchestrator = UnifiedQualityOrchestrator(
            quality_evaluator=quality_evaluator,
            qa_agent=qa_agent,
            qa_bridge=qa_bridge,
            persistence=persistence
        )

        result = await orchestrator.evaluate_content(
            content=generated_content,
            post=blog_post,
            context=evaluation_context
        )
    """

    def __init__(
        self,
        quality_evaluator,  # QualityEvaluator instance
        qa_agent,  # QAAgent instance
        qa_bridge,  # QAAgentBridge instance
        persistence,  # QualityScorePersistence instance
    ):
        self.quality_evaluator = quality_evaluator
        self.qa_agent = qa_agent
        self.qa_bridge = qa_bridge
        self.persistence = persistence

        logger.info(
            "Initializing UnifiedQualityOrchestrator with:"
            "\n  - QualityEvaluator (pattern-based)"
            "\n  - QAAgent (LLM-based binary approval)"
            "\n  - QAAgentBridge (hybrid scoring)"
            "\n  - Persistence layer (database)"
        )

    async def evaluate_content(
        self,
        content: str,
        post: Any,  # BlogPost dataclass
        context: Optional[Dict[str, Any]] = None,
        use_qa_agent: bool = True,
        use_pattern_eval: bool = True,
        store_result: bool = True,
    ) -> Dict[str, Any]:
        """
        Complete quality evaluation workflow.

        Runs both evaluation methods and returns hybrid scoring:
        - Pattern-based: 7-criteria objective scoring (fast)
        - QA Agent: Binary approval with LLM feedback (accurate)
        - Hybrid: Weighted combination (robust)

        Args:
            content: The content to evaluate
            post: BlogPost object with metadata
            context: Optional context (topic, keywords, audience)
            use_qa_agent: Whether to use QA Agent evaluation
            use_pattern_eval: Whether to use pattern-based evaluation
            store_result: Whether to persist result to database

        Returns:
            Dict containing:
            {
                "content": str,
                "pattern_evaluation": {...},
                "qa_evaluation": {
                    "approved": bool,
                    "feedback": str
                },
                "hybrid_evaluation": {...},
                "passing": bool,
                "feedback": str,
                "recommendations": [...],
                "evaluation_method": str,
                "timestamp": datetime
            }
        """

        logger.info(
            f"Starting unified evaluation: " f"pattern={use_pattern_eval}, qa={use_qa_agent}"
        )

        results = {
            "content_preview": content[:100] + "...",
            "evaluation_method": "unified",
            "timestamp": datetime.now(),
        }

        try:
            # Step 1: Pattern-based evaluation (fast, parallel)
            pattern_result = None
            if use_pattern_eval:
                logger.info("Step 1: Running pattern-based evaluation...")
                pattern_result = await self.quality_evaluator.evaluate(
                    content=content,
                    context=context,
                    use_llm=False,  # Use fast pattern matching only
                )
                results["pattern_evaluation"] = {
                    "clarity": pattern_result.clarity,
                    "accuracy": pattern_result.accuracy,
                    "completeness": pattern_result.completeness,
                    "relevance": pattern_result.relevance,
                    "seo_quality": pattern_result.seo_quality,
                    "readability": pattern_result.readability,
                    "engagement": pattern_result.engagement,
                    "overall": pattern_result.overall_score,
                    "feedback": pattern_result.feedback[:200] if pattern_result.feedback else "",
                }
                logger.info(f"Pattern eval: {pattern_result.overall_score}/10")

            # Step 2: QA Agent evaluation (LLM-based)
            qa_result = None
            if use_qa_agent:
                logger.info("Step 2: Running QA Agent evaluation...")
                try:
                    qa_approved, qa_feedback = self.qa_agent.run(
                        post=post, previous_content=content
                    )
                    qa_result = {"approved": qa_approved, "feedback": qa_feedback}
                    results["qa_evaluation"] = qa_result
                    logger.info(f"QA Agent: {'✅ APPROVED' if qa_approved else '⚠️ NEEDS REVISION'}")
                except Exception as e:
                    logger.warning(
                        f"QA Agent evaluation failed: {e}. Continuing with pattern only."
                    )
                    qa_result = None
                    results["qa_evaluation"] = {"error": str(e)}

            # Step 3: Hybrid evaluation (combine both)
            hybrid_result = None
            if pattern_result and qa_result:
                logger.info("Step 3: Creating hybrid evaluation...")
                hybrid_result = await self.qa_bridge.create_hybrid_evaluation(
                    content=content,
                    qa_approved=qa_result["approved"],
                    qa_feedback=qa_result["feedback"],
                    pattern_scores={
                        "clarity": pattern_result.clarity,
                        "accuracy": pattern_result.accuracy,
                        "completeness": pattern_result.completeness,
                        "relevance": pattern_result.relevance,
                        "seo_quality": pattern_result.seo_quality,
                        "readability": pattern_result.readability,
                        "engagement": pattern_result.engagement,
                    },
                    pattern_overall=pattern_result.overall_score,
                    context=context,
                )

                results["hybrid_evaluation"] = {
                    "overall": hybrid_result.hybrid_overall,
                    "qa_weight": hybrid_result.qa_weight,
                    "pattern_weight": hybrid_result.pattern_weight,
                    "feedback": hybrid_result.synthesis_feedback,
                    "recommendations": hybrid_result.recommendations,
                }
                results["passing"] = hybrid_result.passing
                results["feedback"] = hybrid_result.synthesis_feedback
                results["recommendations"] = hybrid_result.recommendations
                results["evaluation_method"] = "hybrid"

                logger.info(
                    f"Hybrid score: {hybrid_result.hybrid_overall}/10 "
                    f"({'✅ PASS' if hybrid_result.passing else '⚠️ NEEDS WORK'})"
                )

                # Step 4: Persist result
                if store_result:
                    logger.info("Step 4: Storing evaluation result...")
                    try:
                        quality_score_dict = self.qa_bridge.to_quality_score_format(hybrid_result)
                        stored_result = await self.persistence.create_quality_score(
                            content_id=None, **quality_score_dict  # Could be post ID if available
                        )
                        results["stored"] = {
                            "id": stored_result.get("id"),
                            "timestamp": stored_result.get("timestamp"),
                        }
                        logger.info(f"Stored evaluation: ID={stored_result.get('id')}")
                    except Exception as e:
                        logger.error(f"Failed to store evaluation: {e}")
                        results["storage_error"] = str(e)

            elif pattern_result:
                # Pattern only (no QA Agent)
                logger.info("Using pattern-based evaluation only (no QA Agent)")
                results["passing"] = pattern_result.passing
                results["feedback"] = pattern_result.feedback
                results["recommendations"] = pattern_result.suggestions
                results["evaluation_method"] = "pattern_only"

                if store_result:
                    try:
                        stored_result = await self.persistence.create_quality_score(
                            content_id=None,
                            overall_score=pattern_result.overall_score,
                            clarity=pattern_result.clarity,
                            accuracy=pattern_result.accuracy,
                            completeness=pattern_result.completeness,
                            relevance=pattern_result.relevance,
                            seo_quality=pattern_result.seo_quality,
                            readability=pattern_result.readability,
                            engagement=pattern_result.engagement,
                            passing=pattern_result.passing,
                            feedback=pattern_result.feedback,
                            suggestions=pattern_result.suggestions,
                            evaluation_method="pattern_only",
                            evaluation_timestamp=datetime.now(),
                        )
                        results["stored"] = {
                            "id": stored_result.get("id"),
                            "timestamp": stored_result.get("timestamp"),
                        }
                    except Exception as e:
                        logger.error(f"Failed to store pattern evaluation: {e}")

            logger.info(
                f"Evaluation complete: passing={results.get('passing')}, "
                f"method={results.get('evaluation_method')}"
            )
            return results

        except Exception as e:
            logger.error(f"Error in unified evaluation: {e}")
            return {
                "error": str(e),
                "passing": False,
                "feedback": "Evaluation failed",
                "evaluation_method": "error",
            }

    async def evaluate_and_refine(
        self,
        content: str,
        post: Any,
        context: Optional[Dict[str, Any]] = None,
        max_refinements: int = 2,
    ) -> Dict[str, Any]:
        """
        Evaluate content and automatically refine if score is too low.

        Workflow:
        1. Evaluate content
        2. If passing: return result
        3. If failing: request refinement from content agent
        4. Re-evaluate refined content
        5. Repeat up to max_refinements times

        Args:
            content: Content to evaluate and potentially refine
            post: BlogPost object
            context: Evaluation context
            max_refinements: Max refinement iterations (default: 2)

        Returns:
            Dict with final evaluation result
        """

        logger.info(f"Starting evaluate_and_refine (max_refinements={max_refinements})")

        current_content = content
        evaluation_history = []

        for iteration in range(max_refinements + 1):
            logger.info(f"Iteration {iteration + 1}: Evaluating content...")

            result = await self.evaluate_content(
                content=current_content, post=post, context=context, store_result=True
            )

            evaluation_history.append(
                {
                    "iteration": iteration + 1,
                    "score": result.get("hybrid_evaluation", {}).get("overall"),
                    "passing": result.get("passing"),
                    "method": result.get("evaluation_method"),
                    "feedback": result.get("feedback", "")[:100],
                }
            )

            # If passing, we're done
            if result.get("passing"):
                logger.info(f"✅ Content passed at iteration {iteration + 1}")
                result["evaluation_history"] = evaluation_history
                return result

            # If not passing and iterations remain
            if iteration < max_refinements:
                logger.info(
                    f"⚠️ Content needs refinement "
                    f"(score: {result.get('hybrid_evaluation', {}).get('overall', 'N/A')}/10)"
                )

                # Get recommendations for refinement
                recommendations = result.get("recommendations", [])
                if not recommendations:
                    logger.warning("No recommendations provided. Cannot refine further.")
                    break

                # Note: In production, this would call the content agent's refinement method
                # For now, we log that refinement is needed
                logger.info(f"Refinement recommendations: {recommendations}")

                # Placeholder for content refinement logic
                # In production: current_content = await content_agent.refine(...)
                # For now, we exit the loop since we can't auto-refine
                break

        # If we've exhausted refinement attempts
        logger.warning(f"Content evaluation incomplete after {iteration + 1} iterations")
        result["evaluation_history"] = evaluation_history
        return result


async def get_unified_orchestrator(
    quality_evaluator=Depends(),  # Would be injected by FastAPI
    qa_agent=Depends(),
    qa_bridge=Depends(),
    persistence=Depends(),
) -> UnifiedQualityOrchestrator:
    """
    Dependency injection function for FastAPI.

    Usage in FastAPI route:
        @router.post("/api/evaluate")
        async def evaluate_endpoint(
            content_data: ContentInput,
            orchestrator: UnifiedQualityOrchestrator = Depends(get_unified_orchestrator)
        ):
            result = await orchestrator.evaluate_content(...)
    """
    return UnifiedQualityOrchestrator(
        quality_evaluator=quality_evaluator,
        qa_agent=qa_agent,
        qa_bridge=qa_bridge,
        persistence=persistence,
    )
