# ğŸ” Comprehensive Workflow Analysis & Debugging Guide

## Glad Labs AI Co-Founder Agent (FastAPI Service)

**Last Updated:** February 10, 2026  
**Status:** Production-Ready | Multi-Agent Orchestration | Self-Critiquing Pipeline  
**Purpose:** Complete trace of workflows and debugging techniques for the FastAPI backend

---

## ğŸ“‹ Table of Contents

1. [System Architecture Overview](#system-architecture-overview)
2. [Complete Request Flow Diagram](#complete-request-flow-diagram)
3. [Key Entry Points & Routes](#key-entry-points--routes)
4. [Core Services & Responsibilities](#core-services--responsibilities)
5. [Content Generation Pipeline (6-Phase)](#content-generation-pipeline-6-phase)
6. [Execution Workflows](#execution-workflows)
7. [Error Handling & Debugging](#error-handling--debugging)
8. [Database Interactions](#database-interactions)
9. [Quality Assessment Loop](#quality-assessment-loop)
10. [Status Lifecycle Management](#status-lifecycle-management)
11. [Agent Orchestration](#agent-orchestration)
12. [Performance Monitoring](#performance-monitoring)
13. [Debugging Techniques & Commands](#debugging-techniques--commands)

---

## System Architecture Overview

### Three-Tier Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Presentation Layer                                              â”‚
â”‚  - React Oversight Hub (port 3001)                               â”‚
â”‚  - Next.js Public Site (port 3000)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ REST API (HTTP/WebSocket)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Orchestrator (port 8000) - MAIN FOCUS                  â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Routes Layer (20+ modules)                               â”‚   â”‚
â”‚  â”‚ â”œâ”€ task_routes.py        (Task CRUD, status mgmt)       â”‚   â”‚
â”‚  â”‚ â”œâ”€ workflow_routes.py    (Workflow execution)           â”‚   â”‚
â”‚  â”‚ â”œâ”€ agents_routes.py      (Agent status/commands)        â”‚   â”‚
â”‚  â”‚ â”œâ”€ model_routes.py       (LLM provider selection)       â”‚   â”‚
â”‚  â”‚ â”œâ”€ chat_routes.py        (Real-time chat)              â”‚   â”‚
â”‚  â”‚ â”œâ”€ content_routes.py     (Content generation)           â”‚   â”‚
â”‚  â”‚ â””â”€ [15+ other routes]    (Analytics, webhooks, etc.)    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â–²                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Services Layer (60+ modules)                            â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚ ORCHESTRATION:                                           â”‚   â”‚
â”‚  â”‚ â”œâ”€ unified_orchestrator.py  (Master request router)    â”‚   â”‚
â”‚  â”‚ â”œâ”€ workflow_router.py       (Workflow execution)       â”‚   â”‚
â”‚  â”‚ â”œâ”€ task_intent_router.py    (NLP intent parsing)       â”‚   â”‚
â”‚  â”‚ â””â”€ pipeline_executor.py     (Task chaining engine)     â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚ EXECUTION:                                               â”‚   â”‚
â”‚  â”‚ â”œâ”€ task_executor.py         (Background task runner)    â”‚   â”‚
â”‚  â”‚ â”œâ”€ content_router_service.py(Content generation)        â”‚   â”‚
â”‚  â”‚ â”œâ”€ langgraph_orchestrator.py(LangGraph execution)      â”‚   â”‚
â”‚  â”‚ â””â”€ prompt_manager.py        (Unified prompt library)    â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚ QUALITY:                                                 â”‚   â”‚
â”‚  â”‚ â”œâ”€ quality_service.py       (QA framework, 7 criteria)  â”‚   â”‚
â”‚  â”‚ â”œâ”€ content_critique_loop.py (Self-critiquing)          â”‚   â”‚
â”‚  â”‚ â””â”€ qa_agent_bridge.py       (QA agent integration)     â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚ PERSISTENCE:                                             â”‚   â”‚
â”‚  â”‚ â”œâ”€ database_service.py      (Unified DB coordinator)    â”‚   â”‚
â”‚  â”‚ â”œâ”€ tasks_db.py              (Task CRUD)                â”‚   â”‚
â”‚  â”‚ â”œâ”€ content_db.py            (Content operations)        â”‚   â”‚
â”‚  â”‚ â””â”€ users_db.py              (User management)           â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚ ROUTING:                                                 â”‚   â”‚
â”‚  â”‚ â”œâ”€ model_router.py          (LLM provider selection)    â”‚   â”‚
â”‚  â”‚ â””â”€ command_queue.py         (Task queueing)             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â–²                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ SQL (asyncpg)
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Layer                                                       â”‚
â”‚                                                                   â”‚
â”‚  PostgreSQL Database (primary persistence)                       â”‚
â”‚  â”œâ”€ tasks table              (Task records, status)              â”‚
â”‚  â”œâ”€ content table            (Generated content)                 â”‚
â”‚  â”œâ”€ users table              (User accounts)                     â”‚
â”‚  â”œâ”€ task_status_history      (Audit trail)                       â”‚
â”‚  â”œâ”€ workflow_history         (Execution records)                 â”‚
â”‚  â”œâ”€ quality_scores           (Quality metrics)                   â”‚
â”‚  â””â”€ [10+ other tables]       (Settings, webhooks, etc.)         â”‚
â”‚                                                                   â”‚
â”‚  Redis Cache (optional)                                           â”‚
â”‚  â””â”€ Session cache, rate limiting                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Complete Request Flow Diagram

### Request Lifecycle (End-to-End)

```
User Request (REST/WebSocket)
        â”‚
        â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Route Handler       â”‚  (task_routes.py, etc.)
    â”‚ - Authentication    â”‚
    â”‚ - Input validation  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ UnifiedOrchestrator.process_request()   â”‚
    â”‚ Step 1: Parse & Route Request           â”‚
    â”‚ - Extract intent from user input        â”‚
    â”‚ - Determine request type                â”‚
    â”‚ - Route to appropriate handler          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Step 2: Extract Intent & Parameters        â”‚
    â”‚ - NLP parsing (TaskIntentRouter)           â”‚
    â”‚ - Pattern matching                         â”‚
    â”‚ - Parameter normalization                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Step 3: Create Task Record                  â”‚
    â”‚ - DatabaseService.create_task()            â”‚
    â”‚ - Initial status: "pending"                â”‚
    â”‚ - Store in PostgreSQL                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Step 4: Queue for Background Execution     â”‚
    â”‚ - TaskExecutor._process_loop() detects    â”‚
    â”‚ - Status â†’ "in_progress"                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PHASE 1: Content Generation                         â”‚
    â”‚ TaskExecutor._execute_task()                        â”‚
    â”‚ â”œâ”€ Research â†’ Extract context, gather data          â”‚
    â”‚ â”œâ”€ Creative â†’ Generate content with brand voice     â”‚
    â”‚ â”œâ”€ QA â†’ Critique quality                            â”‚
    â”‚ â”œâ”€ Refine â†’ Apply feedback                          â”‚
    â”‚ â”œâ”€ Images â†’ Select/generate visuals                 â”‚
    â”‚ â””â”€ Format â†’ Prepare for publishing                  â”‚
    â”‚                                                       â”‚
    â”‚ Generated via: UnifiedOrchestrator                   â”‚
    â”‚ Fallback: AIContentGenerator (if orchestrator down) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PHASE 2: Quality Validation (ContentCritiqueLoop)  â”‚
    â”‚ - Evaluate against 7 criteria:                      â”‚
    â”‚   1. Clarity & Readability                          â”‚
    â”‚   2. Brand Voice Match                              â”‚
    â”‚   3. SEO Optimization                               â”‚
    â”‚   4. Engagement Score                               â”‚
    â”‚   5. Fact Accuracy                                  â”‚
    â”‚   6. Grammar & Style                                â”‚
    â”‚   7. Length & Completeness                          â”‚
    â”‚                                                      â”‚
    â”‚ - Pass threshold (default 0.7)?                     â”‚
    â”‚   YES â†’ Move to approval                            â”‚
    â”‚   NO  â†’ Attempt refinement loop                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PHASE 3: Approval Gate                              â”‚
    â”‚ - Status â†’ "awaiting_approval"                      â”‚
    â”‚ - Notify user via WebSocket/webhook                 â”‚
    â”‚ - Wait for user action:                             â”‚
    â”‚   âœ“ Approve  â†’ Move to publishing                   â”‚
    â”‚   âœ— Reject   â†’ Store feedback, retrigger generation â”‚
    â”‚   âš™ Modify   â†’ Apply user changes                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PHASE 4: Publishing                               â”‚
    â”‚ - Format content per CMS requirements              â”‚
    â”‚ - Generate SEO metadata                            â”‚
    â”‚ - Convert to markdown/HTML                         â”‚
    â”‚ - Post to CMS (Strapi) via webhook                 â”‚
    â”‚ - Status â†’ "published"                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PHASE 5: Training Data Capture                     â”‚
    â”‚ - Store input â†’ output pair                        â”‚
    â”‚ - Record quality metrics                           â”‚
    â”‚ - Log refinement attempts                          â”‚
    â”‚ - Use for future model fine-tuning                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PHASE 6: Completion & Analytics                    â”‚
    â”‚ - Final status: "completed"                        â”‚
    â”‚ - Calculate costs (token usage)                     â”‚
    â”‚ - Record execution time                            â”‚
    â”‚ - Update user analytics                            â”‚
    â”‚ - Broadcast completion to clients                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Entry Points & Routes

### Primary API Routes

| Route | Method | Purpose | Status Code | Handler |
|-------|--------|---------|-------------|---------|
| `/api/tasks` | POST | Create new task | 201 | `create_task()` |
| `/api/tasks` | GET | List tasks (paginated) | 200 | `list_tasks()` |
| `/api/tasks/{id}` | GET | Get task details | 200 | `get_task()` |
| `/api/tasks/{id}/status` | PUT | Update task status | 200 | `update_task_status_*()` |
| `/api/tasks/{id}/approve` | POST | Approve task | 200 | `approve_task()` |
| `/api/tasks/{id}/publish` | POST | Publish task | 200 | `publish_task()` |
| `/api/tasks/{id}/reject` | POST | Reject task | 200 | `reject_task()` |
| `/api/tasks/intent` | POST | Create from NL intent | 201 | `create_task_from_intent()` |
| `/api/tasks/confirm-intent` | POST | Confirm & execute task | 201 | `confirm_and_execute_task()` |
| `/api/health` | GET | Service health | 200 | FastAPI health check |
| `/api/models` | GET | Available LLM models | 200 | `get_available_models()` |
| `/ws/tasks/{id}` | WebSocket | Real-time task progress | - | `websocket_task_progress()` |

### Complete Route Files

```
routes/
â”œâ”€â”€ task_routes.py              [2,623 lines] â† MAIN: Task CRUD + status lifecycle
â”œâ”€â”€ agents_routes.py             [~500 lines] â† Agent status and commands
â”œâ”€â”€ model_routes.py              [~300 lines] â† LLM model selection/health
â”œâ”€â”€ chat_routes.py               [~400 lines] â† Real-time chat/streaming
â”œâ”€â”€ content_routes.py            [~600 lines] â† Content generation shortcuts
â”œâ”€â”€ workflow_history.py          [~500 lines] â† Execution history queries
â”œâ”€â”€ analytics_routes.py          [~800 lines] â† Metrics and statistics
â”œâ”€â”€ command_queue_routes.py      [~400 lines] â† Task queueing
â”œâ”€â”€ bulk_task_routes.py          [~300 lines] â† Batch operations
â”œâ”€â”€ cms_routes.py                [~500 lines] â† CMS (Strapi) integration
â”œâ”€â”€ webhooks.py                  [~400 lines] â† External webhook handling
â”œâ”€â”€ websocket_routes.py          [~300 lines] â† WebSocket connections
â”œâ”€â”€ auth_unified.py              [~400 lines] â† Authentication/OAuth
â”œâ”€â”€ settings_routes.py           [~500 lines] â† User settings
â”œâ”€â”€ social_routes.py             [~400 lines] â† Social media publishing
â”œâ”€â”€ newsletter_routes.py         [~300 lines] â† Email newsletter
â”œâ”€â”€ media_routes.py              [~300 lines] â† File uploads/media
â”œâ”€â”€ writing_style_routes.py      [~600 lines] â† RAG writing samples
â”œâ”€â”€ privacy_routes.py            [~200 lines] â† GDPR/privacy endpoints
â”œâ”€â”€ metrics_routes.py            [~400 lines] â† Real-time metrics
â””â”€â”€ ollama_routes.py             [~200 lines] â† Local Ollama models
```

---

## Core Services & Responsibilities

### 1. Orchestration Services

#### `UnifiedOrchestrator` (1,066 lines)

**Location:** `services/unified_orchestrator.py`

**Primary Responsibility:** Master request router and executor

**Key Methods:**

- `process_request(user_input, context)` - Main entry point
- `process_command_async(command, context)` - Legacy command processing
- `_parse_request(user_input, request_id, context)` - Parse and route
- `_extract_intent_and_params(request)` - NLP intent extraction
- `_route_and_execute(request)` - Route to appropriate handler
- `_assess_quality(result)` - Quality evaluation
- `_refine_if_needed(result)` - Refinement loop
- `_store_training_data(result)` - Training capture

**Request Types Handled:**

```python
RequestType.CONTENT_CREATION          # Blog posts, articles, copy
RequestType.CONTENT_SUBTASK           # Individual content stages
RequestType.FINANCIAL_ANALYSIS        # Cost analysis, ROI
RequestType.COMPLIANCE_CHECK          # Legal/risk review
RequestType.TASK_MANAGEMENT           # Create/manage tasks
RequestType.INFORMATION_RETRIEVAL     # Data lookups
RequestType.DECISION_SUPPORT          # "What should I..."
RequestType.SYSTEM_OPERATION          # Status, health
RequestType.INTERVENTION              # Manual overrides
```

#### `WorkflowRouter` (varies)

**Location:** `services/workflow_router.py`

**Purpose:** Routes workflows to ModularPipelineExecutor

**Key Methods:**

- `execute_workflow()` - Execute workflow with custom pipeline
- `execute_from_natural_language()` - NL to workflow
- `_parse_intent()` - Intent extraction for workflows

#### `TaskIntentRouter` (varies)

**Location:** `services/task_intent_router.py`

**Purpose:** NLP parsing for user requests

**Key Methods:**

- `route_user_input()` - Main NLP entry point
- `_determine_subtasks()` - Break down task into stages
- `_should_confirm()` - User confirmation needed?
- `_determine_execution_strategy()` - Sequential vs parallel

#### `ModularPipelineExecutor` (472 lines)

**Location:** `services/pipeline_executor.py`

**Purpose:** Execute task pipelines with automatic chaining

**Key Features:**

- Automatic task chaining (output N â†’ input N+1)
- Flexible error handling (fail/skip/retry)
- Checkpoint support for approval workflows
- Complete execution history

**Key Methods:**

- `execute(request: WorkflowRequest)` - Execute pipeline
- `resume_workflow()` - Resume from checkpoint
- `_get_pipeline()` - Get task sequence
- `_merge_task_output()` - Merge task results

---

### 2. Execution Services

#### `TaskExecutor` (1,015 lines)

**Location:** `services/task_executor.py`

**Primary Responsibility:** Background task processing pipeline

**Architecture:**

```
TaskExecutor
â”œâ”€â”€ _process_loop()          â† Main loop (runs every 5 seconds)
â”‚   â””â”€â”€ Polls for pending tasks
â”‚
â”œâ”€â”€ _process_single_task()   â† Process one task
â”‚   â””â”€â”€ Update status â†’ in_progress
â”‚       â””â”€â”€ _execute_task()
â”‚           â”œâ”€â”€ PHASE 1: Content Generation
â”‚           â”œâ”€â”€ PHASE 2: Quality Critique
â”‚           â”œâ”€â”€ PHASE 3: Approval Gate
â”‚           â”œâ”€â”€ PHASE 4: Publishing
â”‚           â”œâ”€â”€ PHASE 5: Training Data
â”‚           â””â”€â”€ PHASE 6: Analytics
â”‚
â”œâ”€â”€ _execute_task()          â† Main execution handler
â”‚   â”œâ”€â”€ Get model for execution
â”‚   â”œâ”€â”€ Call UnifiedOrchestrator.process_request()
â”‚   â”œâ”€â”€ Validate content via critique loop
â”‚   â”œâ”€â”€ Attempt refinement if needed
â”‚   â”œâ”€â”€ Store result
â”‚   â””â”€â”€ Update task status
â”‚
â”œâ”€â”€ _fallback_generate_content() â† Fallback if orchestrator unavailable
â””â”€â”€ get_stats()              â† Execution statistics
```

**Key Constants:**

```python
TASK_TIMEOUT_SECONDS = 600  # 10 minutes per task
POLL_INTERVAL = 5           # Check for tasks every 5 seconds
MAX_REFINEMENT_ATTEMPTS = 3
QUALITY_THRESHOLD = 0.7     # Minimum quality score (70%)
```

#### `LangGraphOrchestrator` (varies)

**Location:** `services/langgraph_orchestrator.py`

**Purpose:** LangGraph-based workflow execution (advanced pipeline)

**Key Methods:**

- `execute_content_pipeline()` - Execute with/without streaming
- `_sync_execution()` - Synchronous execution
- `_stream_execution()` - Streaming execution

#### `ContentRouterService` (varies)

**Location:** `services/content_router_service.py`

**Purpose:** Multi-phase content generation orchestration

**Key Methods:**

- `process_content_generation_task()` - Main entry point
- Executes research â†’ creative â†’ qa â†’ refine â†’ images â†’ publish

---

### 3. Quality & Critique Services

#### `ContentCritiqueLoop` (varies)

**Location:** `services/content_critique_loop.py`

**Purpose:** Self-critiquing quality assessment

**Evaluation Criteria (7):**

1. **Clarity & Readability** - Easy to understand?
2. **Brand Voice Match** - Consistent with style?
3. **SEO Optimization** - Good for search?
4. **Engagement Score** - Compelling content?
5. **Fact Accuracy** - Truthful claims?
6. **Grammar & Style** - Well-written?
7. **Length & Completeness** - Sufficient depth?

**Key Methods:**

- `critique()` - Perform quality assessment
- Returns: `{passed, score, feedback, needs_refinement}`

#### `UnifiedQualityService` (varies)

**Location:** `services/quality_service.py`

**Purpose:** Comprehensive QA framework

**Features:**

- Multi-criteria evaluation
- Custom scoring rubrics
- Feedback generation
- Threshold-based acceptance

---

### 4. Database Services

#### `DatabaseService` (coordination layer)

**Location:** `services/database_service.py`

**Purpose:** Unified database interface coordinator

**Delegates to:**

```python
DatabaseService
â”œâ”€â”€ UsersDatabase          â†’ User accounts, OAuth, auth
â”œâ”€â”€ TasksDatabase          â†’ Task CRUD, filtering, status
â”œâ”€â”€ ContentDatabase        â†’ Posts, quality scores, metrics
â”œâ”€â”€ AdminDatabase          â†’ Logging, finance, settings
â”œâ”€â”€ WritingStyleDatabase   â†’ Writing samples for RAG
â””â”€â”€ [5+ other modules]     â†’ Migrations, schema, health
```

**Key Methods:**

```python
# Task operations
await db.create_task(task_data)
await db.get_task(task_id)
await db.update_task_status(task_id, new_status)
await db.get_pending_tasks(limit=10)
await db.get_task_status_history(task_id)

# Content operations
await db.store_content(task_id, content_data)
await db.get_content_by_task(task_id)

# Quality operations
await db.store_quality_score(task_id, score, criteria)
await db.get_quality_by_task(task_id)
```

---

### 5. AI & Model Services

#### `ModelRouter` (varies)

**Location:** `services/model_router.py`

**Purpose:** Intelligent LLM provider selection with cost optimization

**Selection Priority (Fallback Chain):**

```
1. Ollama (local, ~$0, ~20ms latency)
   â””â”€ If unavailable:
   
2. Anthropic Claude (configurable model)
   â””â”€ If key missing or unavailable:
   
3. OpenAI GPT (configurable model)
   â””â”€ If key missing or unavailable:
   
4. Google Gemini
   â””â”€ If key missing or unavailable:
   
5. Echo/Mock Response (dev/demo)
```

**Key Methods:**

- `select_model(cost_tier)` - Get best model for tier
- `get_available_models()` - List all available
- `validate_model(model_name)` - Check availability
- `get_model_cost(model, tokens)` - Calculate cost

#### `PromptManager` (varies)

**Location:** `services/prompt_manager.py`

**Purpose:** Centralized prompt library and management

**Prompt Categories:**

```
Content Generation:
â”œâ”€ research_prompt
â”œâ”€ creative_prompt
â”œâ”€ qa_prompt
â”œâ”€ refine_prompt
â”œâ”€ images_prompt
â””â”€ format_prompt

System Prompts:
â”œâ”€ quality_rubric
â”œâ”€ critique_prompt
â””â”€ refinement_prompt
```

---

## Content Generation Pipeline (6-Phase)

### Complete Phase Breakdown

```
REQUEST RECEIVED
      â”‚
      â–¼
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PHASE 1: RESEARCH
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Responsibility: Gather background, identify key points
  
  Agent: Research Agent
  Time: ~5-10 seconds
  Cost: $0.05-0.10
  
  Inputs:
  - topic: str              (e.g., "AI Trends 2026")
  - target_length: int      (e.g., 2000)
  - target_audience: str    (e.g., "tech professionals")
  
  Process:
  1. Search for relevant information
  2. Gather context and sources
  3. Identify key points and themes
  4. Summarize findings
  
  Outputs:
  - research_data: Dict     (key findings, sources)
  - context_summary: str    (condensed research)
  - key_points: List[str]   (main takeaways)
  
  Error Handling:
  â”œâ”€ No sources found â†’ Return generic research
  â”œâ”€ Search failed â†’ Use cached knowledge
  â””â”€ Timeout â†’ Use fallback research
  
  Debug Points:
  â””â”€ Research completeness check
  â””â”€ Source availability validation


PHASE 2: CREATIVE GENERATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Responsibility: Generate initial draft with brand voice
  
  Agent: Creative Agent
  Time: ~10-15 seconds
  Cost: $0.10-0.20
  
  Inputs:
  - research_data: Dict     (from Phase 1)
  - style: str              (e.g., "informative, engaging")
  - tone: str               (e.g., "professional")
  - brand_voice: str        (RAG from writing samples)
  
  Process:
  1. Load brand voice guidelines from writing samples
  2. Generate initial outline
  3. Write full content with brand voice
  4. Add engaging hooks and transitions
  
  Outputs:
  - draft_content: str      (full article/post)
  - outline: List[str]      (structure)
  - metadata: Dict          (title, excerpt, etc.)
  
  Error Handling:
  â”œâ”€ Writing samples unavailable â†’ Use default voice
  â”œâ”€ Generation timeout â†’ Return partial content
  â””â”€ Invalid parameters â†’ Use defaults
  
  Debug Points:
  â””â”€ Brand voice consistency
  â””â”€ Content length validation
  â””â”€ Prompt template rendering


PHASE 3: QA & CRITIQUE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Responsibility: Evaluation without rewriting
  
  Service: ContentCritiqueLoop
  Time: ~8-12 seconds
  Cost: $0.05-0.15
  
  Inputs:
  - draft_content: str      (from Phase 2)
  - quality_threshold: float (default: 0.7)
  
  Process:
  1. Evaluate against 7 criteria:
     â”œâ”€ Clarity & Readability
     â”œâ”€ Brand Voice Match
     â”œâ”€ SEO Optimization
     â”œâ”€ Engagement Score
     â”œâ”€ Fact Accuracy
     â”œâ”€ Grammar & Style
     â””â”€ Length & Completeness
  
  2. Generate score (0-1)
  3. Create feedback without rewrites
  4. Determine if needs refinement
  
  Outputs:
  - quality_score: float    (0-1)
  - passed: bool            (score >= threshold?)
  - feedback: str           (improvement suggestions)
  - needs_refinement: bool  (attempt refinement?)
  - criteria_scores: Dict   (breakdown by criterion)
  
  Error Handling:
  â”œâ”€ QA agent unavailable â†’ Skip critique
  â”œâ”€ Scoring failed â†’ Default to 0.5
  â””â”€ Invalid feedback â†’ Use generic feedback
  
  Debug Points:
  â””â”€ Criterion scoring accuracy
  â””â”€ Threshold comparison
  â””â”€ Feedback relevance


PHASE 4: REFINEMENT (Conditional)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Responsibility: Apply feedback, improve draft
  
  Agent: Creative Agent (again)
  Time: ~10-15 seconds (if needed)
  Cost: $0.10-0.20 (if needed)
  Triggered: Only if Phase 3 fails OR needs_refinement=true
  Max Attempts: 3
  
  Inputs:
  - original_content: str   (from Phase 2)
  - critique_feedback: str  (from Phase 3)
  
  Process:
  1. Read feedback from Phase 3
  2. Rewrite content incorporating suggestions
  3. Maintain brand voice
  4. Return refined version
  5. Loop back to Phase 3 if score still low
  
  Loop Logic:
  â”œâ”€ Attempt 1: Quality < 0.7
  â”‚  â”œâ”€ Refine â†’ GOTO Phase 3
  â”‚  â””â”€ Phase 3: Pass? â†’ Proceed : Attempt 2
  â”œâ”€ Attempt 2: Still failing
  â”‚  â”œâ”€ Stronger critique â†’ Refine
  â”‚  â””â”€ GOTO Phase 3
  â”œâ”€ Attempt 3: Final attempt
  â”‚  â””â”€ Max retries reached â†’ Continue (may publish with warning)
  
  Error Handling:
  â”œâ”€ Refinement timeout â†’ Use original content
  â”œâ”€ Agent error â†’ Skip refinement
  â””â”€ Max retries exceeded â†’ Continue anyway
  
  Debug Points:
  â””â”€ Refinement quality improvements
  â””â”€ Loop iteration count
  â””â”€ Feedback application success


PHASE 5: IMAGE GENERATION & SELECTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Responsibility: Visuals for publication
  
  Agents: Image Agent + Selection Engine
  Time: ~15-30 seconds
  Cost: $0.20-1.00 (depending on generation)
  
  Process:
  1. Extract key topics from content
  2. Generate image description
  3. Search free image library (Pexels)
  4. If no match â†’ Generate image (DALL-E/Midjourney)
  5. Select best result
  6. Generate alt-text (SEO + accessibility)
  7. Prepare metadata
  
  Inputs:
  - content: str            (from Phase 2/4)
  - topic: str             (original request)
  
  Outputs:
  - featured_image: str     (URL or path)
  - alt_text: str          (accessibility + SEO)
  - image_metadata: Dict    (title, description, credits)
  - thumbnail: str         (optional, for preview)
  
  Error Handling:
  â”œâ”€ No images found â†’ Use default placeholder
  â”œâ”€ Generation failed â†’ Use stock image
  â”œâ”€ Timeout â†’ Use cached images
  â””â”€ Invalid input â†’ Skip images
  
  Debug Points:
  â””â”€ Image quality and relevance
  â””â”€ Alt-text generation quality
  â””â”€ Generation vs. library search time


PHASE 6: FORMATTING & PUBLISHING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Responsibility: CMS preparation and publishing
  
  Agent: Publishing Agent
  Time: ~5-10 seconds
  Cost: $0.05 (mostly metadata generation)
  
  Process:
  1. Validate content structure
  2. Convert to markdown (GitHub-flavored)
  3. Generate SEO metadata:
     â”œâ”€ Meta description (155 chars)
     â”œâ”€ Keywords (5-10)
     â”œâ”€ Open Graph tags
     â””â”€ Twitter card tags
  4. Add structured data (JSON-LD)
  5. Format for CMS (Strapi)
  6. Queue for publishing
  
  Inputs:
  - content: str            (from Phase 2/4)
  - metadata: Dict          (title, excerpt, author)
  - featured_image: str     (from Phase 5)
  
  Outputs:
  - formatted_content: str  (markdown)
  - seo_metadata: Dict      (SEO fields)
  - cms_payload: Dict       (Strapi format)
  - published_url: str      (after publishing)
  
  Status Updates:
  â””â”€ awaiting_approval â†’ pending_publishing â†’ published
  
  Error Handling:
  â”œâ”€ CMS connection failed â†’ Queue for retry
  â”œâ”€ Invalid payload â†’ Fix and retry
  â”œâ”€ Rate limited â†’ Backoff and retry
  â””â”€ Permanent error â†’ Status = failed
  
  Debug Points:
  â””â”€ CMS payload validation
  â””â”€ SEO metadata quality
  â””â”€ Publishing status tracking


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COMPLETION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Status: completed
  Metrics Recorded:
  â”œâ”€ Total execution time (Phase 1-6)
  â”œâ”€ Total cost (sum of all phases)
  â”œâ”€ Refinement attempts
  â”œâ”€ Quality score (final)
  â”œâ”€ Times consumed by each phase
  â””â”€ Any errors or warnings
  
  Training Data Captured:
  â”œâ”€ User input â†’ generated output
  â”œâ”€ All quality scores
  â”œâ”€ Refinement feedback
  â””â”€ Final published content
  
  Stored in:
  â””â”€ PostgreSQL for future model fine-tuning
```

---

## Execution Workflows

### Authentication & Request Validation

```python
# 1. Route Handler (task_routes.py)
@router.post("/api/tasks")
async def create_task(
    request_body: TaskCreateRequest,
    current_user: User = Depends(get_current_user)  # â† Auth check
) -> UnifiedTaskResponse:
    # 2. Input Validation (Pydantic)
    # - request_body automatically validated
    
    # 3. Check permissions
    # - current_user verified by get_current_user()
    
    # 4. Create initial task
    task_id = await database.create_task({
        'user_id': current_user.id,
        'topic': request_body.topic,
        'status': 'pending',
        ...
    })
    
    # 5. Return response
    return UnifiedTaskResponse(id=task_id, status='pending')
```

### Natural Language Intent Routing

```python
# From UnifiedOrchestrator.process_request()

async def process_request(user_input: str, context: Dict):
    # 1. Parse and Route
    request = await self._parse_request(user_input)
    
    # 2. Determine Type
    if "create" in user_input and "blog" in user_input:
        request.request_type = RequestType.CONTENT_CREATION
    elif "analyze" in user_input and "financial" in user_input:
        request.request_type = RequestType.FINANCIAL_ANALYSIS
    # ... etc
    
    # 3. Extract Intent & Parameters
    intent = await self._extract_intent_and_params(request)
    # â†’ topic, style, tone, target_length, etc.
    
    # 4. Route to Handler
    if request.request_type == RequestType.CONTENT_CREATION:
        result = await self._handle_content_creation(intent)
    elif request.request_type == RequestType.FINANCIAL_ANALYSIS:
        result = await self._handle_financial_analysis(intent)
    # ... else branches
    
    # 5. Assess Quality
    quality = await self._assess_quality(result)
    
    # 6. Refine if Needed
    if quality['score'] < 0.7:
        result = await self._refine_if_needed(result, quality)
    
    # 7. Store Training Data
    await self._store_training_data(
        input=user_input,
        output=result,
        quality=quality
    )
    
    return result
```

### Background Task Processing Loop

```python
# From TaskExecutor._process_loop()

async def _process_loop():
    while self.running:
        try:
            # 1. Poll for pending tasks
            pending = await database.get_pending_tasks(limit=10)
            
            if pending:
                logger.info(f"Found {len(pending)} pending tasks")
                
                # 2. Process each task
                for task in pending:
                    await self._process_single_task(task)
            
            # 3. Await next poll
            await asyncio.sleep(self.poll_interval)  # 5 seconds
            
        except Exception as e:
            logger.error(f"Error in process loop: {e}")
            await asyncio.sleep(self.poll_interval)
```

---

## Error Handling & Debugging

### Error Hierarchy

```
Exception
â”œâ”€â”€ HTTPException (FastAPI)
â”‚   â”œâ”€â”€ 400 Bad Request         (invalid input)
â”‚   â”œâ”€â”€ 401 Unauthorized        (authentication failed)
â”‚   â”œâ”€â”€ 403 Forbidden           (permission denied)
â”‚   â”œâ”€â”€ 404 Not Found           (resource missing)
â”‚   â”œâ”€â”€ 409 Conflict            (status transition invalid)
â”‚   â””â”€â”€ 503 Service Unavailable (orchestrator down)
â”‚
â”œâ”€â”€ DatabaseError
â”‚   â”œâ”€â”€ ConnectionError         (PostgreSQL unavailable)
â”‚   â”œâ”€â”€ IntegrityError          (constraint violation)
â”‚   â””â”€â”€ OperationalError        (query failed)
â”‚
â”œâ”€â”€ OrchestrationError
â”‚   â”œâ”€â”€ ModelNotAvailable       (no LLM providers)
â”‚   â”œâ”€â”€ TimeoutError            (execution too long)
â”‚   â””â”€â”€ ParsingError            (intent extraction failed)
â”‚
â””â”€â”€ ApplicationError
    â”œâ”€â”€ ConfigError             (missing env vars)
    â”œâ”€â”€ ServiceInitError        (startup failed)
    â””â”€â”€ ContentValidationError  (generated content invalid)
```

### Global Exception Handlers

```python
# From utils/exception_handlers.py
register_exception_handlers(app)

Registered Handlers:
â”œâ”€ log_exceptions()           â†’ Log all errors
â”œâ”€ http_exception_handler()   â†’ Return JSON errors
â”œâ”€ validation_exception_handler() â†’ Pydantic errors
â”œâ”€ database_exception_handler() â†’ SQL errors
â”œâ”€ orchestrator_exception_handler() â†’ Orchestration errors
â””â”€ general_exception_handler() â†’ Catch-all
```

---

## Database Interactions

### Table Structure

```sql
-- Core Tables
tasks
â”œâ”€ id UUID PRIMARY KEY
â”œâ”€ user_id UUID                    (owner)
â”œâ”€ task_name VARCHAR                 (e.g., "Blog Post")
â”œâ”€ topic VARCHAR                     (e.g., "AI Trends")
â”œâ”€ status VARCHAR                    (pending, in_progress, etc.)
â”œâ”€ created_at TIMESTAMP              (creation time)
â”œâ”€ started_at TIMESTAMP              (execution start)
â”œâ”€ completed_at TIMESTAMP            (execution end)
â”œâ”€ task_metadata JSONB               (topic, style, tone, etc.)
â”œâ”€ result JSONB                      (generated content)
â”œâ”€ quality_score FLOAT               (0-1)
â””â”€ error_message TEXT                (if failed)

task_status_history
â”œâ”€ id UUID PRIMARY KEY
â”œâ”€ task_id UUID FOREIGN KEY
â”œâ”€ old_status VARCHAR
â”œâ”€ new_status VARCHAR
â”œâ”€ changed_by UUID                   (user who made change)
â”œâ”€ reason TEXT                       (why changed)
â””â”€ changed_at TIMESTAMP

content
â”œâ”€ id UUID PRIMARY KEY
â”œâ”€ task_id UUID FOREIGN KEY
â”œâ”€ content_type VARCHAR              (blog_post, social, etc.)
â”œâ”€ title VARCHAR
â”œâ”€ body TEXT                         (markdown)
â”œâ”€ summary TEXT
â”œâ”€ featured_image VARCHAR            (URL)
â”œâ”€ seo_metadata JSONB                (meta desc, keywords, etc.)
â”œâ”€ quality_criteria JSONB            (7 criteria scores)
â”œâ”€ created_at TIMESTAMP
â””â”€ published_at TIMESTAMP

quality_scores
â”œâ”€ id UUID PRIMARY KEY
â”œâ”€ task_id UUID FOREIGN KEY
â”œâ”€ criteria_scores JSONB             (clarity, brand_voice, etc.)
â”œâ”€ overall_score FLOAT               (0-1)
â”œâ”€ feedback TEXT
â”œâ”€ timestamp TIMESTAMP
â””â”€ version INT                       (which iteration)

workflow_history
â”œâ”€ id UUID PRIMARY KEY
â”œâ”€ user_id UUID
â”œâ”€ workflow_type VARCHAR             (content_generation, etc.)
â”œâ”€ start_time TIMESTAMP
â”œâ”€ end_time TIMESTAMP
â”œâ”€ status VARCHAR                    (completed, failed, etc.)
â”œâ”€ task_results JSONB                (all task outputs)
â”œâ”€ metrics JSONB                     (timing, costs, etc.)
â””â”€ errors JSONB                      (any errors encountered)
```

### Common Queries

```python
# Get pending tasks
async def get_pending_tasks(limit=10):
    query = """
        SELECT * FROM tasks
        WHERE status = 'pending'
        ORDER BY created_at ASC
        LIMIT $1
    """
    return await pool.fetch(query, limit)

# Update task status with history
async def update_task_status(task_id, new_status, reason=None):
    # 1. Get old status
    old_task = await get_task(task_id)
    
    # 2. Update task
    await db.execute("""
        UPDATE tasks SET status = $1,
        WHERE id = $2
    """, new_status, task_id)
    
    # 3. Record history
    await db.execute("""
        INSERT INTO task_status_history
        (task_id, old_status, new_status, reason)
        VALUES ($1, $2, $3, $4)
    """, task_id, old_task['status'], new_status, reason)

# Store content with quality metrics
async def store_content(task_id, content_data, quality_score):
    await db.execute("""
        INSERT INTO content
        (task_id, title, body, featured_image, seo_metadata)
        VALUES ($1, $2, $3, $4, $5::JSONB)
    """, task_id, content_data['title'], ...)
    
    await db.execute("""
        INSERT INTO quality_scores
        (task_id, overall_score, criteria_scores, feedback)
        VALUES ($1, $2, $3::JSONB, $4)
    """, task_id, quality_score['score'], ...)
```

---

## Quality Assessment Loop

### 7-Criteria Evaluation Framework

```python
QUALITY_CRITERIA = {
    'clarity': {
        'weight': 0.15,
        'threshold': 0.7,
        'description': 'Is content easy to understand?',
        'indicators': [
            'Clear sentence structure',
            'Logical paragraph flow',
            'Technical terms explained',
            'Short paragraphs (<100 words)'
        ]
    },
    'brand_voice': {
        'weight': 0.15,
        'threshold': 0.7,
        'description': 'Does it match brand voice?',
        'indicators': [
            'Uses characteristic phrases',
            'Maintains tone consistency',
            'Follows style guidelines',
            'References brand values'
        ]
    },
    'seo': {
        'weight': 0.15,
        'threshold': 0.7,
        'description': 'Is it SEO optimized?',
        'indicators': [
            'Keyword placement natural',
            'Meta description quality',
            'Heading structure proper',
            'Internal link opportunities'
        ]
    },
    'engagement': {
        'weight': 0.15,
        'threshold': 0.7,
        'description': 'Is it compelling?',
        'indicators': [
            'Strong opening hook',
            'Calls-to-action clear',
            'Examples and data included',
            'Visually scannable'
        ]
    },
    'accuracy': {
        'weight': 0.15,
        'threshold': 0.7,
        'description': 'Are facts correct?',
        'indicators': [
            'Claims verifiable',
            'Statistics cited',
            'No contradictions',
            'Current information'
        ]
    },
    'grammar': {
        'weight': 0.10,
        'threshold': 0.7,
        'description': 'Is writing quality high?',
        'indicators': [
            'No grammatical errors',
            'Proper punctuation',
            'Correct spelling',
            'Good word choice'
        ]
    },
    'completeness': {
        'weight': 0.15,
        'threshold': 0.7,
        'description': 'Is it sufficiently detailed?',
        'indicators': [
            'Meets length target',
            'Covers main topics',
            'Provides adequate depth',
            'Conclusion present'
        ]
    }
}

# Overall calculation
overall_score = (
    clarity_score * 0.15 +
    brand_voice_score * 0.15 +
    seo_score * 0.15 +
    engagement_score * 0.15 +
    accuracy_score * 0.15 +
    grammar_score * 0.10 +
    completeness_score * 0.15
)

# Pass/Fail
threshold = 0.7  # 70%
passed = overall_score >= threshold
```

### Refinement Loop Logic

```python
async def execute_with_refinement(task, max_attempts=3):
    """
    Execute content generation with automatic refinement.
    """
    attempt = 0
    current_content = None
    quality_history = []
    
    while attempt < max_attempts:
        attempt += 1
        logger.info(f"Refinement attempt {attempt}/{max_attempts}")
        
        # 1. Generate (or refine if iteration > 1)
        if attempt == 1:
            current_content = await orchestrator.generate_content(task)
        else:
            # Pass previous critique to guide refinement
            refinement_prompt = {
                'original': current_content,
                'feedback': quality_history[-1]['feedback'],
                'failed_criteria': quality_history[-1]['failed']
            }
            current_content = await orchestrator.refine_content(refinement_prompt)
        
        # 2. Critique
        quality = await critique_loop.critique(current_content)
        quality_history.append(quality)
        
        logger.info(f"Quality score: {quality['score']:.2f}")
        logger.info(f"Criteria: {quality['criteria_scores']}")
        
        # 3. Check if passed
        if quality['passed']:
            logger.info(f"âœ… Content passed after {attempt} attempt(s)")
            return {
                'content': current_content,
                'quality': quality,
                'attempts': attempt,
                'history': quality_history
            }
        
        # 4. If final attempt, warn but continue
        if attempt == max_attempts:
            logger.warning(
                f"âš ï¸  Max refinement attempts ({max_attempts}) reached"
            )
            logger.warning(f"   Final score: {quality['score']:.2f}")
            logger.warning(f"   Continuing with current content")
            return {
                'content': current_content,
                'quality': quality,
                'attempts': attempt,
                'history': quality_history,
                'warning': 'Max refinements reached, publishing anyway'
            }
    
    return None  # Should not reach
```

---

## Status Lifecycle Management

### Valid Status Transitions

```
State Machine (Task Status)

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   pending   â”‚  â† Created state
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                â”‚ (auto: TaskExecutor picks up)
                â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  in_progress     â”‚  â† Execution
         â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
             â”‚     â”‚    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼              â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ failed  â”‚  â”‚awaiting_    â”‚  â”‚ completed      â”‚
â”‚         â”‚  â”‚approval     â”‚  â”‚ (but not pub)  â”‚
â”‚(error)  â”‚  â””â”€â”¬â”€â”€â”€â”¬â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚  â”‚            â”‚
        â”‚      â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚      â”‚   â”‚               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”˜   â–¼               â–¼
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         â”‚  rejected       â”‚  â”‚published â”‚
â”‚         â”‚ (user rejected) â”‚  â”‚(final)   â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚    â–²                            â–²
â””â”€â”€â”€â”€â”˜                            â”‚
    (retry loop)       (publish endpoint)


Valid Transitions:
pending           â†’ in_progress   (auto)
in_progress       â†’ completed     (success)
in_progress       â†’ failed        (error)
completed         â†’ awaiting_approval (auto)
awaiting_approval â†’ approved      (user action)
awaiting_approval â†’ rejected      (user action)
approved          â†’ pending_publishing (auto)
pending_publishing â†’ published    (webhook)
rejected          â†’ pending       (retry)
failed            â†’ pending       (manual retry)
*                 â†’ cancelled     (user cancel)
```

### Status Change Validation

```python
class StatusTransitionValidator:
    
    # Allowed transitions per status
    TRANSITIONS = {
        'pending': ['in_progress', 'cancelled'],
        'in_progress': ['completed', 'failed', 'cancelled'],
        'completed': ['awaiting_approval', 'cancelled'],
        'awaiting_approval': ['approved', 'rejected', 'cancelled'],
        'approved': ['pending_publishing', 'cancelled'],
        'pending_publishing': ['published', 'failed'],
        'published': ['cancelled'],
        'rejected': ['pending'],
        'failed': ['pending', 'cancelled'],
        'cancelled': []
    }
    
    @staticmethod
    def is_valid(current: str, target: str) -> bool:
        """Check if transition allowed"""
        allowed = TRANSITIONS.get(current, [])
        return target in allowed
    
    @staticmethod
    def get_reason(current: str, target: str) -> str:
        """Get human-readable reason for status change"""
        reasons = {
            ('pending', 'in_progress'): 'Executor picked up task',
            ('in_progress', 'completed'): 'Content generation complete',
            ('in_progress', 'failed'): 'Generation failed',
            ('completed', 'awaiting_approval'): 'Awaiting user approval',
            ('awaiting_approval', 'approved'): 'User approved content',
            ('approved', 'pending_publishing'): 'Publishing to CMS',
            ('pending_publishing', 'published'): 'Successfully published',
            # ... etc
        }
        return reasons.get((current, target), '')
```

---

## Agent Orchestration

### Agent Fleet Architecture

```
UnifiedOrchestrator (Master)
â”‚
â”œâ”€ Content Agent                    â”œâ”€ Research Sub-Agent
â”‚  (6-stage pipeline)               â”œâ”€ Creative Sub-Agent
â”‚                                   â”œâ”€ QA Sub-Agent
â”‚                                   â””â”€ Publishing Sub-Agent
â”‚
â”œâ”€ Financial Agent                  â”œâ”€ Cost Calculator
â”‚  (ROI & budgeting)                â”œâ”€ Historical Analyzer
â”‚                                   â””â”€ Forecaster
â”‚
â”œâ”€ Market Insight Agent             â”œâ”€ Trend Analyzer
â”‚  (industry analysis)              â”œâ”€ Competitor Scout
â”‚                                   â””â”€ Report Generator
â”‚
â””â”€ Compliance Agent                 â”œâ”€ Legal Checker
   (risk assessment)                â”œâ”€ Policy Validator
                                    â””â”€ Recommendation Engine
```

### Agent Interface

```python
class Agent(ABC):
    """Base agent interface"""
    
    async def execute(self, request: Dict) -> Dict:
        """
        Execute agent task.
        
        Args:
            request: {
                'type': 'content_generation|financial_analysis|...',
                'parameters': {...},
                'context': {...}
            }
        
        Returns:
            {
                'status': 'success|failed',
                'output': {...},
                'error': '...',
                'cost_usd': 0.50,
                'duration_ms': 5000,
                'metadata': {...}
            }
        """
        pass
```

### Agent Execution Patterns

```python
# Parallel Execution (when independent)
results = await asyncio.gather(
    content_agent.execute(request),
    financial_agent.execute(financial_request),
    market_agent.execute(market_request),
)

# Sequential Execution (when dependent)
# 1. Research phase
research = await content_agent.research(topic)

# 2. Creative phase (uses research)
creative = await content_agent.create(
    research_data=research,
    style=style
)

# 3. QA phase (uses creative)
quality = await content_agent.critique(creative)

# 4. Refine phase (uses QA feedback)
if not quality['passed']:
    refined = await content_agent.refine(
        content=creative,
        feedback=quality['feedback']
    )
```

---

## Performance Monitoring

### Key Metrics

```python
# From TaskExecutor.get_stats()
{
    'task_count': 150,           # Total tasks processed
    'success_count': 145,        # Successfully completed
    'error_count': 5,            # Failed tasks
    'published_count': 140,      # Published to CMS
    'avg_execution_time': 45.2,  # Seconds per task
    'total_cost': 235.50,        # USD spent
    'quality_avg': 0.82,         # Average quality score
    'uptime_hours': 72.5,        # Hours running
    'queue_size': 3,             # Pending tasks
}

# Per-Phase Timing
phase_timing = {
    'research': 7500,            # ms
    'creative': 12000,           # ms
    'qa': 9500,                  # ms
    'refinement': 0,             # ms (if not needed)
    'images': 20000,             # ms
    'format': 3500,              # ms
    'total': 52500               # ms
}

# Cost Breakdown
costs = {
    'research': 0.07,
    'creative': 0.18,
    'qa': 0.08,
    'refinement': 0.00,
    'images': 0.50,
    'format': 0.02,
    'total': 0.85                # Total USD for task
}
```

### Monitoring Endpoints

```python
# Health Check
GET /api/health
Returns: {
    'status': 'healthy|degraded|unhealthy',
    'database': 'connected|disconnected',
    'orchestrator': 'initialized|failed',
    'task_executor': 'running|stopped',
    'pending_tasks': 3,
    'uptime_seconds': 86400,
}

# Executor Statistics
GET /api/executor/stats
Returns: (see above)

# System Metrics
GET /api/metrics
Returns: {
    'requests_total': 1500,
    'requests_active': 5,
    'latency_p95': 450,           # ms
    'latency_p99': 1200,          # ms
    'error_rate': 0.033,          # 3.3%
}

# Workflow History
GET /api/workflow-history?limit=100
Returns: [{
    'workflow_id': '...',
    'type': 'content_generation',
    'user_id': '...',
    'status': 'completed',
    'start_time': '2026-02-10T10:00:00Z',
    'end_time': '2026-02-10T10:02:30Z',
    'duration_seconds': 150,
    'task_count': 6,
    'total_cost': 0.85,
}]
```

---

## Debugging Techniques & Commands

### 1. Enable Debug Logging

```python
# In main.py or startup
import logging
logging.basicConfig(
    level=logging.DEBUG,        # Changed from INFO
    format='%(asctime)s [%(name)s] %(levelname)s: %(message)s'
)

# Or via environment variable
export LOG_LEVEL=debug
```

### 2. Tail Live Logs

**Terminal 1: Backend**

```bash
# If using npm run dev
npm run dev:cofounder 2>&1 | grep -E "TASK_EXECUTE|TASK_SINGLE|PHASE"

# Or with Python directly
python -m uvicorn main:app --log-level debug --reload
```

**Terminal 2: Grep specific task**

```bash
npm run dev:cofounder 2>&1 | grep "task_id_here"
```

### 3. Monitor Database Queries

```python
# Enable SQL query logging
export SQL_DEBUG=true

# In .env.local:
SQL_DEBUG=true
```

Watch logs for query patterns:

```
SELECT * FROM tasks WHERE status = 'pending'
UPDATE tasks SET status = 'in_progress' WHERE id = '...'
INSERT INTO task_status_history (task_id, old_status, new_status, ...)
```

### 4. Trace Workflow Execution

**Create a custom test task:**

```bash
curl -X POST http://localhost:8000/api/tasks \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "task_name": "Debug Task",
    "topic": "Test Topic",
    "style": "informative",
    "tone": "professional",
    "target_length": 1000,
    "generate_featured_image": false
  }'

# Returns: {"id": "task_uuid", "status": "pending"}
```

**Poll task status:**

```bash
# Watch in real-time
while true; do
  curl -s http://localhost:8000/api/tasks/task_uuid \
    -H "Authorization: Bearer YOUR_TOKEN" | jq .status
  sleep 2
done
```

**Check execution logs:**

```bash
# Find in logs:
# [TASK_EXEC_LOOP] Found 1 pending task(s)
# [TASK_SINGLE] Starting task processing
# [TASK_EXECUTE] PHASE 1: Generating content via orchestrator...
# [TASK_EXECUTE] PHASE 2: Validating content through critique loop...
# [TASK_EXECUTE] PHASE 3: Content approved
# etc.
```

### 5. Check Orchestrator Status

```bash
# Get model availability
curl -s http://localhost:8000/api/models \
  -H "Authorization: Bearer YOUR_TOKEN" | jq .

# Returns: {
#   "providers": {
#     "ollama": "available",
#     "anthropic": "available",
#     "openai": "unavailable",
#     ...
#   },
#   "selected": "ollama"
# }

# Get orchestrator stats (if endpoint exists)
curl -s http://localhost:8000/api/executor/stats | jq .
```

### 6. Debug Critical Failures

**If tasks stuck in "in_progress":**

```python
# Check if TaskExecutor is running
# From logs: "âœ… Task executor background processor started"

# If not running:
# 1. Check startup logs for errors
# 2. Look for: "ğŸ›‘ StartupManager" errors
# 3. Verify orchestrator initialized

# Manual reset (development only):
# 1. Check database directly:
curl -s http://localhost:5432  # PostgreSQL

# 2. Find stuck task:
psql $DATABASE_URL -c "
SELECT id, status, started_at
FROM tasks
WHERE status = 'in_progress'
AND started_at < NOW() - INTERVAL '1 hour'
"

# 3. Reset manually (if needed):
psql $DATABASE_URL -c "
UPDATE tasks
SET status = 'failed',
    error_message = 'Manually reset - executor issue'
WHERE id = 'task_id'
"
```

**If orchestrator crashes:**

```python
# Look for in logs:
# "âŒ Orchestrator initialization failed"
# "âŒ Error initializing UnifiedOrchestrator"

# Check prerequisites:
# 1. Is database connected?
#    â†’ Check DATABASE_URL in .env.local
#    â†’ Verify PostgreSQL running

# 2. Are LLM keys set?
#    â†’ Check OPENAI_API_KEY, ANTHROPIC_API_KEY, etc.
#    â†’ At least one should be set

# 3. Is Ollama running (if using local)?
#    â†’ ollama serve (in another terminal)
#    â†’ curl http://localhost:11434/api/tags

# 4. Check service initialization order
#    â†’ Look for startup logs showing each service init
```

### 7. Profile Execution Time

```python
# TaskExecutor logs timing per phase
# Look for in logs:
# âœ… [TASK_EXECUTE] PHASE 1 Complete (generation): Generated 1524 chars in 12.3s
# ğŸ” [TASK_EXECUTE] PHASE 2 Complete (critique): Quality score 0.82
# ğŸ“ [TASK_EXECUTE] PHASE 4 Complete (images): Selected/generated image in 18.5s
# etc.

# Calculate bottleneck:
# If PHASE 5 (images) takes 30+ seconds consistently
#  â†’ Optimize image generation or use library instead

# If PHASE 1 (research) takes 20+ seconds
#  â†’ May need cheaper/faster model or cached knowledge base
```

### 8. Test Individual Phases

**Create minimal test:**

```python
# In a test file or Python REPL

from services.task_executor import TaskExecutor
from services.database_service import DatabaseService
from services.unified_orchestrator import UnifiedOrchestrator

# Initialize services
db = DatabaseService()
orchestrator = UnifiedOrchestrator(db)
executor = TaskExecutor(db, orchestrator)

# Test Phase 1 (content generation)
task = {
    'id': 'test-task-1',
    'topic': 'Test Topic',
    'style': 'informative',
    'tone': 'professional',
    'target_length': 500
}

# Run generation
result = await orchestrator.process_request(
    topic=task['topic'],
    style=task['style'],
    tone=task['tone'],
    target_length=task['target_length']
)

print(result)  # See output, any errors, timing
```

### 9. WebSocket Debugging

**Connect to real-time progress:**

```javascript
// In browser console or test client

const taskId = 'your-task-uuid';
const ws = new WebSocket(`ws://localhost:8000/ws/tasks/${taskId}`);

ws.onmessage = (event) => {
    const data = JSON.parse(event.data);
    console.log('Progress update:', data);
    // Shows current phase, progress %, estimated time, etc.
};

ws.onerror = (error) => {
    console.error('WebSocket error:', error);
};
```

### 10. Database Inspection

```bash
# Connect to PostgreSQL
psql $DATABASE_URL

# See pending tasks and their status
SELECT id, task_name, status, created_at, started_at
FROM tasks
ORDER BY created_at DESC
LIMIT 10;

# Check task execution history
SELECT task_id, old_status, new_status, reason, changed_at
FROM task_status_history
WHERE task_id = 'your-task-id'
ORDER BY changed_at;

# Check quality scores
SELECT task_id, overall_score, criteria_scores, feedback
FROM quality_scores
WHERE task_id = 'your-task-id'
ORDER BY timestamp DESC;

# See workflow execution times
SELECT workflow_type, status,
       EXTRACT(EPOCH FROM (end_time - start_time)) as duration_seconds,
       metrics
FROM workflow_history
WHERE user_id = 'your-user-id'
ORDER BY start_time DESC
LIMIT 20;
```

---

## Quick Reference: Common Debug Scenarios

| Scenario | Issue | Debug Steps |
|----------|-------|------------|
| **Tasks stuck in pending** | Executor not running | Check logs for `Task executor background processor started` |
| **Very slow content gen** | Model selection issue | Check `/api/models` endpoint |
| **Quality always fails** | Threshold too high | Check ContentCritiqueLoop, reduce threshold temporarily |
| **Refinement loop endless** | Max retries not working | Check `MAX_REFINEMENT_ATTEMPTS` constant |
| **Database connection error** | PostgreSQL unavailable | Verify DATABASE_URL, check `psql $DATABASE_URL` |
| **Orchestrator crashes** | Service init failure | Check startup logs, verify all dependencies |
| **WebSocket disconnects** | Connection issue | Check CORS settings, WebSocket timeout |
| **Image gen timeout** | Image service slow | Check image provider status, use library instead |
| **CMS publish fails** | Strapi unavailable | Verify CMS_URL, check webhook auth |
| **Memory leak** | Long-running executor | Check for unclosed connections in loops |

---

## Summary

This comprehensive analysis provides:

âœ… **Complete System Architecture** - Three-tier structure with all major components  
âœ… **Request Flow Diagrams** - End-to-end request lifecycle  
âœ… **6-Phase Pipeline** - Detailed breakdown of content generation  
âœ… **Service Map** - 60+ services with responsibilities  
âœ… **Error Handling** - Exception hierarchy and recovery  
âœ… **Database Schema** - Table structure and queries  
âœ… **Quality Framework** - 7-criteria evaluation system  
âœ… **Debugging Techniques** - 10 practical debugging approaches  
âœ… **Performance Metrics** - Monitoring and profiling  
âœ… **Quick Reference** - Common scenarios and solutions  

Use this guide to trace any request through the system, identify bottlenecks, and debug issues systematically.

---

**For questions or updates, see:**

- `/docs/05-AI_AGENTS_AND_INTEGRATION.md` - Agent details
- `/docs/06-OPERATIONS_AND_MAINTENANCE.md` - Operations guide  
- `src/cofounder_agent/README.md` - Service README
- `src/cofounder_agent/DOCUMENTATION_INDEX.md` - Full documentation index
