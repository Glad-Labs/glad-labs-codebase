# âš¡ Ollama Model Selector - Quick Reference

**Status:** âœ… Complete (Nov 1, 2025)  
**Build:** âœ… 0 errors/warnings  
**Testing:** Ready

---

## ğŸ¯ What Changed

**Problem:** `âš ï¸ Model 'mistral' not found`

**Solution:**

- âœ… Settings page with model dropdown
- âœ… Shows all 16+ available models
- âœ… Persists selection to browser storage
- âœ… Auto warm-up when changed

---

## ğŸ“ Files Modified

```
Backend:
  src/cofounder_agent/routes/ollama_routes.py
    + POST /api/ollama/select-model (validates & selects model)

Frontend:
  web/oversight-hub/src/OversightHub.jsx
    + Settings page with dropdown
    + Model list display
    + localStorage persistence
    + handleOllamaModelChange() function
```

---

## ğŸš€ How to Use

### 1. Open Settings

```
Menu â˜° â†’ âš™ï¸ Settings
```

### 2. Select Model

```
ğŸ¤– Select Ollama Model
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ mistral:latest   â–¼   â”‚ â† Click dropdown
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Select from:
â€¢ mistral:latest
â€¢ qwq:latest
â€¢ qwen3:14b
â€¢ neural-chat:latest
... (12 more models)
```

### 3. Chat Uses Selected Model

Model automatically used for all chat messages

---

## âœ¨ Features

| Feature               | Status |
| --------------------- | ------ |
| View all models       | âœ… Yes |
| Select model          | âœ… Yes |
| Remember selection    | âœ… Yes |
| Auto warm-up          | âœ… Yes |
| Validation            | âœ… Yes |
| Error messages        | âœ… Yes |
| Persistence on reload | âœ… Yes |

---

## ğŸ§ª Quick Test

1. Start backend + Ollama
2. Open http://localhost:3001
3. Menu â˜° â†’ Settings âš™ï¸
4. See dropdown with all models
5. Select different model
6. See âœ… confirmation message
7. Refresh page â†’ selection persists âœ“

---

## ğŸ”Œ API Endpoint

```bash
POST /api/ollama/select-model
Content-Type: application/json

Request:
{
  "model": "mistral:latest"
}

Response:
{
  "success": true,
  "selected_model": "mistral:latest",
  "message": "âœ… Model selected",
  "available_models": ["mistral:latest", "qwq:latest", ...],
  "timestamp": "..."
}
```

---

## ğŸ“Š Available Models

```
1.  mistral:latest           (7B general)
2.  qwq:latest              (Fast)
3.  qwen3:14b               (Alibaba)
4.  qwen2.5:14b             (Alibaba)
5.  neural-chat:latest      (Intel)
6.  deepseek-r1:14b         (Reasoning)
7.  llava:latest            (Vision)
8.  mixtral:latest          (MoE)
9.  llama2:latest           (Meta)
10. gemma3:12b              (Google)
11. mixtral:instruct        (Tuned)
12. llava:13b               (Vision)
13. mixtral:8x7b-instruct   (Variant)
14. llama3:70b-instruct     (Large)
15. gemma3:27b              (Google)
16. gpt-oss:20b             (OSS)
```

---

## ğŸ® Controls

| Action          | Location                          |
| --------------- | --------------------------------- |
| Select model    | Settings âš™ï¸ â†’ Dropdown            |
| See available   | Settings âš™ï¸ â†’ List                |
| Change model    | Dropdown â†’ Select â†’ âœ… Auto saves |
| Reset selection | Settings âš™ï¸ â†’ Choose different    |

---

## ğŸ”„ Data Flow

```
User selects model in dropdown
        â†“
handleOllamaModelChange('new-model')
        â†“
POST /api/ollama/select-model
        â†“
Backend validates model exists
        â†“
Frontend: setSelectedOllamaModel()
        â†“
localStorage.setItem('selectedOllamaModel', 'new-model')
        â†“
Chat message uses new model
```

---

## ğŸ’¾ Browser Storage

```
Key: selectedOllamaModel
Value: "mistral:latest"

Persists across:
- Page reloads âœ…
- Browser restarts âœ…
- Tab switches âœ…

Cleared on:
- Clear browser data
- localStorage.clear()
```

---

## ğŸ› Troubleshooting

| Problem           | Solution                           |
| ----------------- | ---------------------------------- |
| Dropdown empty    | Start Ollama: `ollama serve`       |
| "Model not found" | Check spelling, run: `ollama list` |
| Selection lost    | Check localStorage enabled         |
| Wrong model used  | Verify in Settings page            |

---

## ğŸ“š Full Documentation

See: `docs/OLLAMA_MODEL_SELECTOR.md` (comprehensive guide)

---

## âœ… Verification

Before using:

- âœ… Frontend builds: `npm run build` â†’ 0 errors
- âœ… Backend compiles: Python syntax OK
- âœ… Ollama running: `ollama serve`
- âœ… Backend running: `python -m uvicorn main:app --reload`

---

**Production Ready!** ğŸš€ Deploy with confidence.
