# üîå Integration Code Snippets - Ready to Copy/Paste

**Purpose:** Quick reference for integrating quality evaluation into routes  
**Status:** Ready to implement  
**Time to integrate:** 1-2 hours

---

## üìù Step 1: Update content_routes.py (Add Imports)

```python
# At the top of src/cofounder_agent/routes/content_routes.py, add these imports:

from services.quality_evaluator import get_quality_evaluator, QualityScore
from services.quality_score_persistence import get_quality_score_persistence
```

---

## üîÑ Step 2: Wrap Content Generation with Evaluation

### Example: Content Generation Endpoint

**Before (current code):**

```python
@router.post("/api/content/generate-blog-post")
async def generate_blog_post(request: dict):
    # Generate content
    content = await content_agent.generate(request)
    return {"content": content}
```

**After (with evaluation):**

```python
@router.post("/api/content/generate-blog-post")
async def generate_blog_post(
    request: dict,
    model_router = Depends(get_model_router)  # If using LLM evaluation
):
    # 1. Generate content
    content = await content_agent.generate(request)
    content_id = request.get("content_id", f"content-{uuid.uuid4()}")

    # 2. Evaluate quality
    try:
        evaluator = await get_quality_evaluator(model_router=None)  # Use pattern-based
        quality_result = await evaluator.evaluate(
            content=content,
            context={
                "topic": request.get("topic", ""),
                "target_keywords": request.get("keywords", [])
            },
            use_llm=False  # Fast pattern-based evaluation
        )

        # 3. Store evaluation in database
        from services.database_service import get_database_service
        db_service = await get_database_service()
        persistence = await get_quality_score_persistence(db_service)

        stored_result = await persistence.store_evaluation(
            content_id=content_id,
            quality_score=quality_result,
            task_id=request.get("task_id"),
            content_length=len(content.split()),
            context_data={
                "topic": request.get("topic", ""),
                "keywords": request.get("keywords", [])
            }
        )

        # 4. Check if passes quality threshold
        if not quality_result.passing and quality_result.overall_score < 7.0:
            logger.warning(
                f"Content {content_id} failed quality check: "
                f"score {quality_result.overall_score}/10. "
                f"Suggestions: {quality_result.suggestions}"
            )
            # Optional: trigger refinement here
            # await trigger_refinement(content_id, quality_result.feedback)

        # 5. Return response with quality scores
        return {
            "content": content,
            "content_id": content_id,
            "quality_score": {
                "overall": quality_result.overall_score,
                "clarity": quality_result.clarity,
                "accuracy": quality_result.accuracy,
                "completeness": quality_result.completeness,
                "relevance": quality_result.relevance,
                "seo_quality": quality_result.seo_quality,
                "readability": quality_result.readability,
                "engagement": quality_result.engagement,
                "passing": quality_result.passing,
                "feedback": quality_result.feedback,
                "suggestions": quality_result.suggestions
            }
        }

    except Exception as e:
        logger.error(f"Quality evaluation failed: {e}")
        # Return content anyway, but note evaluation failed
        return {
            "content": content,
            "content_id": content_id,
            "quality_score": None,
            "evaluation_error": str(e)
        }
```

---

## üîå Step 3: Create New Evaluation Routes

**Create file:** `src/cofounder_agent/routes/evaluation_routes.py`

```python
"""
Evaluation API Routes
Provides endpoints for quality evaluation results and metrics
"""

from fastapi import APIRouter, HTTPException, Depends
from typing import Optional, Dict, Any
from datetime import date, datetime
import logging

from services.quality_evaluator import get_quality_evaluator, QualityScore
from services.quality_score_persistence import get_quality_score_persistence
from services.database_service import get_database_service

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/evaluation", tags=["evaluation"])


@router.post("/evaluate")
async def manually_evaluate_content(
    content: str,
    topic: Optional[str] = None,
    keywords: Optional[list] = None,
    use_llm: bool = False,
    task_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    Manually evaluate content quality

    Query Parameters:
    - content (str): Content to evaluate
    - topic (str, optional): Topic/subject matter
    - keywords (list, optional): Target keywords
    - use_llm (bool, optional): Use LLM evaluation (default: fast pattern-based)
    - task_id (str, optional): Associated task ID

    Returns:
    - overall_score, all 7 criterion scores, passing status, feedback, suggestions
    """
    try:
        evaluator = await get_quality_evaluator()

        result = await evaluator.evaluate(
            content=content,
            context={
                "topic": topic or "",
                "target_keywords": keywords or []
            },
            use_llm=use_llm
        )

        return {
            "overall_score": result.overall_score,
            "clarity": result.clarity,
            "accuracy": result.accuracy,
            "completeness": result.completeness,
            "relevance": result.relevance,
            "seo_quality": result.seo_quality,
            "readability": result.readability,
            "engagement": result.engagement,
            "passing": result.passing,
            "feedback": result.feedback,
            "suggestions": result.suggestions,
            "evaluation_method": result.evaluation_method,
            "timestamp": result.evaluation_timestamp.isoformat()
        }

    except Exception as e:
        logger.error(f"Evaluation failed: {e}")
        raise HTTPException(status_code=500, detail=f"Evaluation failed: {str(e)}")


@router.get("/results/{content_id}")
async def get_evaluation_results(
    content_id: str,
    limit: int = 10
) -> Dict[str, Any]:
    """
    Get evaluation history for specific content

    Parameters:
    - content_id (str): Content ID to query
    - limit (int, optional): Max results (default: 10)

    Returns:
    - List of evaluations from newest to oldest
    """
    try:
        db_service = await get_database_service()
        persistence = await get_quality_score_persistence(db_service)

        history = await persistence.get_evaluation_history(content_id, limit=limit)

        return {
            "content_id": content_id,
            "evaluation_count": len(history),
            "evaluations": history
        }

    except Exception as e:
        logger.error(f"Failed to get evaluation results: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/summary/{content_id}")
async def get_quality_summary(content_id: str) -> Dict[str, Any]:
    """
    Get comprehensive quality summary for content

    Parameters:
    - content_id (str): Content ID

    Returns:
    - Latest evaluation, history count, improvements, trends
    """
    try:
        db_service = await get_database_service()
        persistence = await get_quality_score_persistence(db_service)

        summary = await persistence.get_content_quality_summary(content_id)

        return summary

    except Exception as e:
        logger.error(f"Failed to get quality summary: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/metrics/daily")
async def get_daily_metrics(
    target_date: Optional[str] = None
) -> Dict[str, Any]:
    """
    Get today's quality metrics and statistics

    Parameters:
    - target_date (str, optional): ISO date (YYYY-MM-DD), defaults to today

    Returns:
    - Daily aggregated metrics
    """
    try:
        db_service = await get_database_service()
        persistence = await get_quality_score_persistence(db_service)

        if target_date:
            target = datetime.fromisoformat(target_date).date()
        else:
            target = date.today()

        metrics = await persistence.get_quality_metrics_for_date(target)

        return {
            "date": target.isoformat(),
            "metrics": metrics
        }

    except Exception as e:
        logger.error(f"Failed to get daily metrics: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/metrics/trend")
async def get_quality_trend(days: int = 7) -> Dict[str, Any]:
    """
    Get quality trend over specified number of days

    Parameters:
    - days (int, optional): Number of days to analyze (default: 7)

    Returns:
    - List of daily metrics, oldest to newest
    """
    try:
        if days < 1 or days > 365:
            raise HTTPException(status_code=400, detail="Days must be 1-365")

        db_service = await get_database_service()
        persistence = await get_quality_score_persistence(db_service)

        trend = await persistence.get_quality_trend(days=days)

        return {
            "days": days,
            "metric_count": len(trend),
            "metrics": trend
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get quality trend: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/statistics")
async def get_evaluation_statistics() -> Dict[str, Any]:
    """
    Get overall evaluation statistics

    Returns:
    - Total evaluations, pass rate, average scores by criterion
    """
    try:
        evaluator = await get_quality_evaluator()
        stats = await evaluator.get_statistics()

        return stats

    except Exception as e:
        logger.error(f"Failed to get statistics: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

---

## üîó Step 4: Register New Routes in main.py

**In `src/cofounder_agent/main.py`, add this import and route registration:**

```python
# Add this import at the top with other route imports:
from routes.evaluation_routes import router as evaluation_router

# Add this line where other routers are registered (around line 100-150):
app.include_router(evaluation_router)
```

---

## üß™ Step 5: Test the Integration

### Test Manually Evaluate Endpoint

```bash
curl -X POST "http://localhost:8000/api/evaluation/evaluate" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "# Article Title\n\nThis is a test article about machine learning...",
    "topic": "Machine Learning",
    "keywords": ["ML", "AI", "deep learning"],
    "use_llm": false
  }'

# Expected Response:
{
  "overall_score": 7.2,
  "clarity": 7.5,
  "accuracy": 7.0,
  "completeness": 7.5,
  "relevance": 7.0,
  "seo_quality": 6.8,
  "readability": 7.3,
  "engagement": 7.0,
  "passing": true,
  "feedback": "Good quality content...",
  "suggestions": ["Add more examples", "..."],
  "evaluation_method": "pattern-based",
  "timestamp": "2025-12-06T..."
}
```

### Test Get Results Endpoint

```bash
curl -X GET "http://localhost:8000/api/evaluation/results/post-123" \
  -H "Content-Type: application/json"

# Expected Response:
{
  "content_id": "post-123",
  "evaluation_count": 3,
  "evaluations": [
    {
      "overall_score": 8.2,
      "clarity": 8.0,
      ...
    },
    ...
  ]
}
```

### Test Get Metrics Endpoint

```bash
curl -X GET "http://localhost:8000/api/evaluation/metrics/daily" \
  -H "Content-Type: application/json"

# Expected Response:
{
  "date": "2025-12-06",
  "metrics": {
    "total_evaluations": 25,
    "passing_count": 21,
    "failing_count": 4,
    "pass_rate": 84,
    "average_score": 7.6,
    ...
  }
}
```

### Test Get Trend Endpoint

```bash
curl -X GET "http://localhost:8000/api/evaluation/metrics/trend?days=7" \
  -H "Content-Type: application/json"

# Expected Response:
{
  "days": 7,
  "metric_count": 7,
  "metrics": [
    {"date": "2025-11-30", "pass_rate": 72, "average_score": 7.1},
    {"date": "2025-12-01", "pass_rate": 75, "average_score": 7.3},
    ...
  ]
}
```

---

## üìã Integration Checklist

- [ ] Add imports to content_routes.py
- [ ] Wrap content generation with evaluate() call
- [ ] Store results with persistence.store_evaluation()
- [ ] Handle evaluation failures gracefully
- [ ] Return quality score in API response
- [ ] Create evaluation_routes.py file
- [ ] Register evaluation_routes in main.py
- [ ] Test manual evaluate endpoint
- [ ] Test results query endpoint
- [ ] Test metrics endpoints
- [ ] Verify database persistence
- [ ] Check for any errors in logs
- [ ] Performance test with load
- [ ] Update API documentation
- [ ] Deploy to staging

---

## üîÑ Automatic Refinement (Optional - Future)

If you want to add automatic refinement on low scores:

```python
# In content_routes.py, after storing evaluation:

if not quality_result.passing and quality_result.overall_score < 7.0:
    logger.info(f"Triggering refinement for {content_id}")

    # Call refinement agent
    refined_content = await content_agent.refine(
        original_content=content,
        feedback=quality_result.feedback,
        suggestions=quality_result.suggestions
    )

    # Re-evaluate refined content
    refined_result = await evaluator.evaluate(
        content=refined_content,
        context={"topic": topic, "target_keywords": keywords}
    )

    # Store improvement
    await persistence.store_improvement(
        content_id=content_id,
        initial_score=quality_result.overall_score,
        improved_score=refined_result.overall_score,
        best_improved_criterion=find_biggest_improvement(quality_result, refined_result),
        changes_made=f"Refinement: {quality_result.suggestions[:2]}"
    )

    # Use refined content if it passes
    if refined_result.passing:
        content = refined_content
        quality_result = refined_result
```

---

## üìö Complete Integration Flow (Diagram)

```
Content Generation Endpoint Called
        ‚Üì
[1] Generate Content
        ‚Üì
[2] Create Quality Evaluator
        ‚Üì
[3] Evaluate Content (7-criteria)
        ‚Üì
[4] Get Database Service & Persistence
        ‚Üì
[5] Store Evaluation in Database
        ‚Üì
[6] Check if Passing (score >= 7.0)
        ‚îú‚Üí YES: Return content + quality scores
        ‚îî‚Üí NO:  Log warning, optionally trigger refinement
        ‚Üì
[7] Return Response with Quality Metadata
```

---

## üéØ Production Readiness After Integration

**After implementing these code snippets:**

- ‚úÖ Auto-evaluation on all content generation
- ‚úÖ Quality scores in API responses
- ‚úÖ Evaluation results queryable via API
- ‚úÖ Daily metrics and trending available
- ‚úÖ Pass/fail gating in place
- ‚úÖ Database persistence working
- ‚úÖ Ready for automatic refinement (optional)
- ‚úÖ Ready for staging deployment

**Production Readiness:** 75-80% ‚Üí 90%+

---

## üìû Questions?

Each code snippet is ready to copy/paste. If you need:

- Help adapting to your specific routes
- Clarification on any component
- Debugging assistance
- Performance optimization

Just let me know!
