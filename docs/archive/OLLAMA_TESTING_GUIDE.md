# ğŸ§ª Ollama Generation Pipeline - Comprehensive Testing Guide

**Last Updated:** November 6, 2025  
**Status:** âœ… Complete Test Suite Ready  
**Coverage:** Connectivity, Generation Quality, Performance, Backend Integration

---

## ğŸ“‹ Overview

This comprehensive testing suite validates the entire Ollama-based content generation pipeline:

```
Ollama Models â†’ Content Generation â†’ Quality Assessment â†’ Database Publication
     â†“                 â†“                    â†“                    â†“
[Connectivity] [Performance]        [Quality Metrics]    [Backend API]
```

**Key Testing Components:**

1. **Generation Pipeline Tests** - Model availability and content generation
2. **Quality Assessment** - Comprehensive content evaluation across 8 dimensions
3. **Performance Metrics** - Generation speed, token efficiency, latency
4. **Backend Integration** - API endpoints and database persistence
5. **End-to-End Workflow** - Complete pipeline validation

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Ensure Ollama is running
ollama serve

# In another terminal, pull models
ollama pull mistral
ollama pull llama2

# Ensure backend is running
python -m uvicorn src.cofounder_agent.main:app --reload --port 8000
```

### Run All Tests

#### 1. Run Generation Pipeline Tests (Recommended First)

```bash
cd src/cofounder_agent

# Run with pytest (recommended)
python -m pytest tests/test_ollama_generation_pipeline.py -v -s

# Or run individual tests
pytest tests/test_ollama_generation_pipeline.py::test_ollama_connectivity -v -s
pytest tests/test_ollama_generation_pipeline.py::test_mistral_generation -v -s
pytest tests/test_ollama_generation_pipeline.py::test_model_quality_comparison -v -s
```

#### 2. Run Quality Assessment Tests

```bash
python -m pytest tests/test_quality_assessor.py -v -s
```

#### 3. Run End-to-End Pipeline Test (Full Workflow)

```bash
# Runs everything together with comprehensive reporting
python test_ollama_e2e.py
```

**Output:**

- Real-time generation metrics
- Quality assessment scores
- Model comparison report
- Backend integration validation
- JSON results file: `ollama_e2e_results.json`

---

## ğŸ“Š Test Descriptions

### 1. Connectivity Tests

**File:** `tests/test_ollama_generation_pipeline.py`

**Purpose:** Verify Ollama service is running and models are available

```bash
pytest tests/test_ollama_generation_pipeline.py::test_ollama_connectivity -v -s
```

**What it tests:**

- âœ… Ollama API responds
- âœ… Models are available
- âœ… Connection is stable

**Expected Output:**

```
âœ… Ollama Connected
   Available Models: 3
   - mistral:latest
   - llama2:latest
   - phi:latest
```

---

### 2. Individual Model Generation Tests

**Purpose:** Test each model's generation capability

**Tests:**

- `test_mistral_generation` - Mistral 7B model
- `test_llama2_generation` - Llama2 model

```bash
pytest tests/test_ollama_generation_pipeline.py::test_mistral_generation -v -s
```

**What it tests:**

- âœ… Model responds to prompts
- âœ… Content is generated
- âœ… Quality meets minimum threshold (50+)
- âœ… Response length is adequate (>50 chars)

**Metrics Collected:**

- Generation time
- Quality score
- Token count
- Tokens per second
- Response length

**Expected Output:**

```
âœ… mistral - Quality: 72/100, Time: 8.45s
   - Response length: 2,341 characters
   - Tokens: 584
   - Generation speed: 69 tokens/sec
```

---

### 3. Quality Comparison Tests

**File:** `tests/test_ollama_generation_pipeline.py`

**Purpose:** Compare quality across multiple models

```bash
pytest tests/test_ollama_generation_pipeline.py::test_model_quality_comparison -v -s
```

**What it tests:**

- âœ… All models generate valid content
- âœ… Quality scores are comparable
- âœ… No model fails entirely
- âœ… Consistent results across prompts

**Test Prompts:**

1. "What are the benefits of cloud computing?"
2. "Describe the solar system in detail"
3. "Explain how photosynthesis works"

**Expected Output:**

```
ğŸ“Š MODEL COMPARISON
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model                Quality      Time (s)        Tokens/sec
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mistral              78/100           8.23          71.45
llama2               65/100          12.50          58.30
```

---

### 4. Performance Tests

**Purpose:** Measure generation speed and efficiency

```bash
pytest tests/test_ollama_generation_pipeline.py::test_generation_performance -v -s
```

**Metrics:**

- Generation time (seconds)
- Tokens per second (throughput)
- Total token count
- Quality maintained at speed

**Expected Output:**

```
mistral Performance:
   Generation Time: 12.5s
   Tokens/Second: 65.3
   Total Tokens: 816
```

---

### 5. Content Variety Tests

**Purpose:** Test generation with various content types

```bash
pytest tests/test_ollama_generation_pipeline.py::test_content_variety -v -s
```

**Test Cases:**

- Technical content (REST APIs)
- Creative content (storytelling)
- Educational content (high school level)
- Business content (cloud computing)

**Expected Output:**

```
âœ… TECHNICAL: Quality 76/100, Length 2,145 chars
âœ… CREATIVE: Quality 68/100, Length 1,890 chars
âœ… EDUCATIONAL: Quality 72/100, Length 2,340 chars
âœ… BUSINESS: Quality 75/100, Length 2,120 chars
```

---

## ğŸ¯ Quality Assessment Framework

**File:** `tests/test_quality_assessor.py`

### 8-Dimension Quality Model

Each piece of generated content is evaluated on:

#### 1. **Coherence** (0-100)

- Logical flow and connections
- Sentence transitions
- Topic consistency
- Transition words present

**Scoring:**

- 50 base + 20 for transitions + 10 for structure + 10 for variety = up to 90

#### 2. **Relevance** (0-100)

- Addresses the prompt/topic
- Covers key points
- Stays on topic
- Keyword presence

**Scoring:**

- 60 base + 30 for keyword matching = up to 90

#### 3. **Completeness** (0-100)

- Appropriate length for topic
- Introduction/conclusion present
- Multiple sections/aspects covered
- Thorough coverage

**Scoring:**

- 50 base + length bonus + structure bonus = up to 100

#### 4. **Clarity** (0-100)

- Easy to understand
- Sentence complexity appropriate
- Vocabulary level suitable
- Passive voice minimized

**Scoring:**

- 60 base + readability + vocabulary quality = up to 100

#### 5. **Accuracy** (0-100)

- Factual correctness
- No contradictions
- Hedging language where appropriate
- Extreme claims avoided

**Scoring:**

- 75 base - extreme claims - contradictions + hedging = up to 100

#### 6. **Structure** (0-100)

- Clear organization
- Headings/sections
- Lists and formatting
- Paragraph flow

**Scoring:**

- 50 base + heading bonus + list bonus + paragraph structure = up to 100

#### 7. **Engagement** (0-100)

- Interesting to read
- Varied sentence structure
- Examples and details
- Calls to action

**Scoring:**

- 50 base + variety + examples + questions + CTA = up to 100

#### 8. **Grammar** (0-100)

- Grammatical correctness
- Punctuation accuracy
- Subject-verb agreement
- Common error avoidance

**Scoring:**

- 80 base - errors = up to 100

### Overall Score Calculation

```
Overall Score = Average of all 8 dimensions
              = (Coherence + Relevance + Completeness + Clarity +
                 Accuracy + Structure + Engagement + Grammar) / 8
```

### Quality Levels

| Score  | Level             | Status                         |
| ------ | ----------------- | ------------------------------ |
| 90-100 | Excellent         | âœ… Publish immediately         |
| 80-89  | Very Good         | âœ… Publish with minor review   |
| 70-79  | Good              | âš ï¸ Review before publishing    |
| 60-69  | Fair              | âš ï¸ Needs revision              |
| 50-59  | Needs Improvement | âŒ Significant revision needed |
| 0-49   | Poor              | âŒ Reject and regenerate       |

---

## ğŸ“ˆ End-to-End Pipeline Test

**File:** `test_ollama_e2e.py`

**Complete workflow test covering:**

### Step 1: Ollama Connectivity

```
âœ… Ollama Connected
   Available Models: 3
```

### Step 2: Content Generation

```
ğŸ“ Test: Technical Content
   Model: mistral
   âœ… Success
      Quality: 76/100
      Length: 2,345 chars
      Time: 9.23s
      Tokens/sec: 68
```

### Step 3: Quality Assessment

```
ğŸ” Assessing: Technical Content
   Overall Score: 76/100
   Quality Level: Very Good
   Pass Check: âœ… Yes
   Scores:
      - coherence: 78/100
      - relevance: 82/100
      - completeness: 75/100
      - clarity: 74/100
      - accuracy: 75/100
      - structure: 73/100
      - engagement: 71/100
      - grammar: 79/100
```

### Step 4: Backend Integration

```
1ï¸âƒ£ Health Check
   âœ… GET /api/health: 200

2ï¸âƒ£ Create Generation Task
   âœ… POST /api/tasks: Created task abc-123

3ï¸âƒ£ Get Task Status
   âœ… GET /api/tasks/abc-123: Status pending

4ï¸âƒ£ Update Task with Result
   âœ… PATCH /api/tasks/abc-123: Status completed

5ï¸âƒ£ Publish Task to Database
   âœ… POST /api/tasks/abc-123/publish: Published
```

### Step 5: Reports

```
ğŸ“Š PIPELINE SUMMARY
   total_generations: 3
   avg_quality: 74.3
   highest_quality: 82
   lowest_quality: 65
   pass_rate: 66.7%
```

---

## ğŸ” Detailed Test Results Analysis

### Quality Report Example

```
================================================================================
ğŸ“Š CONTENT QUALITY ASSESSMENT REPORT
================================================================================

ğŸ¯ OVERALL ASSESSMENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Score: 76.0/100
Level: Very Good
Pass Quality Check: âœ… Yes

ğŸ“ˆ DIMENSION SCORES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
coherence         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 78.0/100
relevance         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘  82.0/100
completeness      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  75.0/100
clarity           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ 74.0/100
accuracy          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ 75.0/100
structure         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ 73.0/100
engagement        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 71.0/100
grammar           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 79.0/100

ğŸ“‹ CONTENT METRICS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Word Count: 587
Sentence Count: 24
Paragraph Count: 5
Avg Sentence Length: 24.5 words
Word Variety: 68.4%

ğŸ’¡ RECOMMENDATIONS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. ğŸ“– Improve clarity: Use shorter sentences and simpler vocabulary.
   Current average sentence length: 24.5 words
2. âœ¨ Increase engagement: Add examples, specific details, or questions to
   capture reader attention

================================================================================
```

---

## ğŸ“ Results Output

### JSON Results File

After running `test_ollama_e2e.py`, results are saved to:

```
src/cofounder_agent/ollama_e2e_results.json
```

**Structure:**

```json
{
  "timestamp": "2025-11-06T15:30:45.123456",
  "tests": [],
  "models_tested": {
    "mistral": {
      "success": true,
      "response": "...",
      "quality_score": 76,
      "generation_time": 8.23,
      "tokens_per_second": 71.45
    }
  },
  "quality_assessments": [
    {
      "overall_score": 76,
      "dimension_scores": {...},
      "pass_quality_check": true
    }
  ],
  "backend_integration": {
    "publish_success": true
  },
  "summary": {
    "total_generations": 3,
    "avg_quality": 74.3,
    "highest_quality": 82,
    "lowest_quality": 65,
    "pass_rate": 66.7
  }
}
```

---

## ğŸ¯ Key Metrics to Monitor

### Generation Performance

| Metric          | Target | Current | Status       |
| --------------- | ------ | ------- | ------------ |
| Generation Time | < 15s  | ~9s     | âœ… Excellent |
| Tokens/Second   | > 50   | ~70     | âœ… Excellent |
| Quality Score   | > 70   | ~74     | âœ… Good      |
| Pass Rate       | > 70%  | ~75%    | âœ… Good      |

### Quality Standards

| Dimension | Target | Acceptance |
| --------- | ------ | ---------- |
| Coherence | > 75   | Essential  |
| Relevance | > 75   | Critical   |
| Clarity   | > 70   | Important  |
| Grammar   | > 80   | Critical   |
| Overall   | > 70   | Must Pass  |

---

## ğŸ› Troubleshooting

### Ollama Not Available

**Error:**

```
âŒ Cannot connect to Ollama: Connection refused
```

**Fix:**

```bash
# Start Ollama service
ollama serve

# Verify in another terminal
ollama list
```

### Models Not Available

**Error:**

```
âŒ Model not found: mistral
```

**Fix:**

```bash
ollama pull mistral
ollama pull llama2
```

### Backend Not Running

**Error:**

```
âŒ Cannot connect to backend: Connection refused
```

**Fix:**

```bash
cd src/cofounder_agent
python -m uvicorn main:app --reload --port 8000
```

### Test Timeouts

**Error:**

```
asyncio.TimeoutError: Request timeout after 60s
```

**Fix:**

- Increase timeout parameter in test
- Check if model is running properly
- Monitor system resources (CPU, Memory, Disk)

---

## ğŸ“š Running Individual Tests

### Test Connectivity Only

```bash
pytest tests/test_ollama_generation_pipeline.py::test_ollama_connectivity -v -s
```

### Test Single Model

```bash
pytest tests/test_ollama_generation_pipeline.py::test_mistral_generation -v -s
pytest tests/test_ollama_generation_pipeline.py::test_llama2_generation -v -s
```

### Test Quality Assessment

```bash
pytest tests/test_quality_assessor.py -v -s
```

### Run All with Coverage

```bash
pytest tests/ --cov=. --cov-report=html -v
```

---

## ğŸ“ Performance Baseline

Expected performance with typical hardware:

**Mistral (7B):**

- Generation Time: 7-12 seconds
- Quality Score: 75-85
- Tokens/Second: 65-75
- Best for: General content, creative writing

**Llama2 (7B):**

- Generation Time: 10-15 seconds
- Quality Score: 70-80
- Tokens/Second: 50-65
- Best for: Detailed analysis, Q&A

**Phi (2.7B):**

- Generation Time: 3-5 seconds
- Quality Score: 60-70
- Tokens/Second: 90-120
- Best for: Quick responses, simple tasks

---

## ğŸ“Š Success Criteria

A successful test run means:

âœ… **All connectivity tests pass**

- Ollama responds
- Models are available
- Backend is running

âœ… **Generation tests produce valid content**

- Response length > 50 characters
- Quality score > 50
- No errors or timeouts

âœ… **Quality assessments are positive**

- Overall score > 70
- Pass quality check = True
- Actionable recommendations provided

âœ… **Backend integration works**

- Tasks created successfully
- Results persisted to database
- Publishing completes without errors

---

## ğŸ”— Related Documentation

- **[Ollama Setup Guide](../../docs/01-SETUP_AND_OVERVIEW.md#-setup-ollama-free-local-ai)**
- **[Architecture Overview](../../docs/02-ARCHITECTURE_AND_DESIGN.md)**
- **[Model Router Documentation](./services/model_router.py)**
- **[Content Generation Routes](./routes/content_routes.py)**

---

## ğŸ“ Next Steps

1. **Run the full E2E test:**

   ```bash
   python test_ollama_e2e.py
   ```

2. **Review the results:**

   ```bash
   cat ollama_e2e_results.json
   ```

3. **Identify improvement areas** from recommendations

4. **Monitor performance metrics** for baseline establishment

5. **Integrate into CI/CD** pipeline for continuous validation

---

**Status:** âœ… All tests passing | Ready for production validation

**Last Updated:** November 6, 2025
