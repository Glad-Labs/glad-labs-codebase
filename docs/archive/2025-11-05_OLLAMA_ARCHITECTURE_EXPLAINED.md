# üß† Ollama Architecture: Chat & Blog Generation

**Last Updated:** November 2, 2025  
**Overview:** How Glad Labs uses Ollama for both real-time chat and async blog post generation

---

## üìä High-Level Architecture

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    OVERSIGHT HUB (Frontend)                     ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Chat Interface                    Blog Generator UI             ‚îÇ
‚îÇ  (Real-time messages)              (Create blog posts)           ‚îÇ
‚îÇ  ‚îî‚îÄ Send message                   ‚îî‚îÄ Enter topic, style, etc   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ                               ‚îÇ
                      ‚îÇ HTTP POST                     ‚îÇ HTTP POST
                      ‚ñº                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   COFOUNDER AGENT (FastAPI Backend)             ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  POST /api/chat              POST /api/content/blog-posts       ‚îÇ
‚îÇ  ‚îî‚îÄ Immediate response       ‚îî‚îÄ Returns task ID immediately    ‚îÇ
‚îÇ     (sync)                      (async in background)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ                               ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚ñº
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ   OLLAMA (Local, Free)     ‚îÇ
                  ‚îÇ                            ‚îÇ
                  ‚îÇ http://localhost:11434     ‚îÇ
                  ‚îÇ                            ‚îÇ
                  ‚îÇ Available Models:          ‚îÇ
                  ‚îÇ - mistral:latest (default) ‚îÇ
                  ‚îÇ - llama2:latest            ‚îÇ
                  ‚îÇ - neural-chat:latest       ‚îÇ
                  ‚îÇ - qwen2.5:14b              ‚îÇ
                  ‚îÇ - ... 12 more models       ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ Comparison: Chat vs Blog Generation

### Chat (Real-Time, Synchronous)

```
FLOW:
Frontend ‚Üí POST /api/chat {message, model: "ollama"} ‚Üí Backend

Backend:
1. Receives request immediately
2. Calls OllamaClient.chat()
3. Waits for response (BLOCKS request)
4. Returns response to frontend

Characteristics:
‚îú‚îÄ Synchronous: User waits for response
‚îú‚îÄ Fast: Models run instantly on local GPU (RTX 5070)
‚îú‚îÄ Typical latency: 1-5 seconds (depends on model size)
‚îú‚îÄ Model used: llama2 (default, good balance)
‚îú‚îÄ Response size: 500 tokens max
‚îî‚îÄ UI: Shows spinner, then response appears
```

**Code Location:** `src/cofounder_agent/routes/chat_routes.py`

```python
@router.post("", response_model=ChatResponse)
async def chat(request: ChatRequest) -> ChatResponse:
    # Model selection
    if request.model == "ollama":
        actual_ollama_model = "llama2"  # Sync with Ollama

        # Direct call - BLOCKS until response
        chat_result = await ollama_client.chat(
            messages=conversations[request.conversationId],
            model=actual_ollama_model,
            temperature=request.temperature or 0.7,
            max_tokens=request.max_tokens or 500
        )
        response_text = chat_result.get("content", "")

    return ChatResponse(response=response_text, model=request.model, ...)
```

---

### Blog Generation (Async, Background Task)

```
FLOW:
Frontend ‚Üí POST /api/content/blog-posts {topic, style, etc} ‚Üí Backend

Backend:
1. Validates request
2. Creates task ID (UUID)
3. Returns task ID IMMEDIATELY to frontend
4. Spawns BACKGROUND task (non-blocking)
5. Frontend polls for status

Background Task:
1. Generates detailed prompt (1500+ words)
2. Calls Ollama (BLOCKS in background)
3. Stores result in task_store
4. Frontend displays "Complete!" when done

Characteristics:
‚îú‚îÄ Asynchronous: Returns immediately with task_id
‚îú‚îÄ Long-running: Can take 1-10 minutes (depends on topic length)
‚îú‚îÄ Model used: mistral:latest (default)
‚îú‚îÄ Response size: 1500-5000 tokens
‚îú‚îÄ Status tracking: Frontend polls /api/content/blog-posts/tasks/{task_id}
‚îî‚îÄ UI: Shows "Generating..." with progress spinner until complete
```

**Code Location:** `src/cofounder_agent/routes/content_generation.py`

```python
@router.post("/blog-posts", response_model=GenerateBlogPostResponse)
async def create_blog_post(
    request: GenerateBlogPostRequest,
    background_tasks: BackgroundTasks
):
    # Create task ID
    task_id = str(uuid.uuid4())

    # Store initial task status
    task_store[task_id] = {
        "status": "pending",
        "created_at": datetime.utcnow().isoformat(),
    }

    # IMPORTANT: Add background task and return IMMEDIATELY
    # This doesn't block the frontend
    background_tasks.add_task(generate_post_background, task_id, request)

    return GenerateBlogPostResponse(
        task_id=task_id,
        status="pending",
        message="Blog generation started..."
    )

# Background task runs independently
async def generate_post_background(task_id: str, request: GenerateBlogPostRequest):
    """
    This runs in the BACKGROUND while frontend polls for status.
    The request/response cycle completes immediately.
    This function can take minutes without blocking the frontend.
    """
    try:
        task_store[task_id]["status"] = "processing"

        # Generate detailed prompt
        prompt = generate_blog_post_prompt(...)

        # Call Ollama (this BLOCKS but it's in background thread)
        content = await call_ollama(prompt)  # Calls mistral:latest

        # Store result
        task_store[task_id]["status"] = "completed"
        task_store[task_id]["result"] = {...}

    except Exception as e:
        task_store[task_id]["status"] = "error"
        task_store[task_id]["error"] = str(e)
```

---

## üéØ Key Design Decisions

### 1. **Why Synchronous Chat?**

‚úÖ **Pro:** Immediate user feedback, conversational flow
‚úÖ **Pro:** Low latency on local GPU
‚ùå **Con:** Blocks request thread during LLM call

**Solution:** Works fine because:

- Ollama is LOCAL (fast GPU access, no network latency)
- Default model (llama2) is relatively small (7B)
- FastAPI uses async, so other requests aren't blocked
- Chat responses limited to 500 tokens (shorter = faster)

**Typical Performance:**

```
Small prompt (< 100 tokens) ‚Üí 1-2 seconds
Medium prompt (100-300 tokens) ‚Üí 2-5 seconds
Large prompt (> 300 tokens) ‚Üí 5-10 seconds
```

---

### 2. **Why Asynchronous Blog Generation?**

‚úÖ **Pro:** Returns immediately (no waiting)
‚úÖ **Pro:** Can handle long-running tasks
‚úÖ **Pro:** Multiple posts can generate in parallel
‚ùå **Con:** Requires frontend polling

**Solution:** Works great because:

- Blog posts are 1500+ words (takes minutes)
- User doesn't wait for response
- Frontend polls status endpoint every 2-5 seconds
- Multiple blog posts can queue and process sequentially

**Typical Performance:**

```
1500-word blog post ‚Üí 2-5 minutes (depends on topic complexity)
2000-word blog post ‚Üí 3-8 minutes
Multiple posts ‚Üí Sequential (one at a time)
```

---

### 3. **Why Different Models?**

| Task     | Model        | Reason                                    |
| -------- | ------------ | ----------------------------------------- |
| **Chat** | llama2 (7B)  | ‚úÖ Fast ‚úÖ Good quality ‚úÖ Conversational |
| **Blog** | mistral (7B) | ‚úÖ Excellent writing ‚úÖ Creative ‚úÖ Fast  |

**Could we use the same model?** Yes! But mistral is slightly slower for chat but better for longer content.

---

## üöÄ Concurrent Handling

### Can Ollama Handle Simultaneous Requests?

```
Scenario 1: User A chats while User B generates blog
‚îú‚îÄ Chat request (sync, 2s) ‚Üí Ollama
‚îî‚îÄ Blog request (async background) ‚Üí Ollama (queued)

Result: ‚úÖ Both work!
- Chat response: 2 seconds (smaller request)
- Blog generation: Starts immediately but Ollama processes sequentially

Scenario 2: Multiple concurrent chat requests
‚îú‚îÄ User A: POST /api/chat ‚Üí Waiting
‚îú‚îÄ User B: POST /api/chat ‚Üí Waiting
‚îî‚îÄ Ollama processes one at a time in queue

Result: ‚úÖ Works but slightly slower
- Each request waits for previous one to finish
- Total time: ~4 seconds each
```

### Current Limitation: No Concurrency Control

**Current State:**

- ‚úÖ Ollama can handle 1-2 concurrent requests
- ‚ùå No queue management (requests just wait)
- ‚ùå No semaphore limiting (potential overload)
- ‚ö†Ô∏è High load could cause timeouts

**Example Problem:**

```
10 users send blog requests simultaneously
‚îú‚îÄ Request 1: Starts immediately
‚îú‚îÄ Request 2: Waits (Ollama busy)
‚îú‚îÄ Request 3: Waits
‚îú‚îÄ ...
‚îú‚îÄ Request 10: Waits in backend queue
‚îî‚îÄ Result: Request 10 could timeout after 5 minutes!

Why? Ollama can't process 10 requests in parallel.
It can run ONE at a time (or maybe 1-2 with smaller models).
```

---

## üîß How They Share Ollama

### Both Routes Use Same Ollama Instance

**Chat Route Flow:**

```python
# routes/chat_routes.py
ollama_client = OllamaClient()  # Shared instance

@router.post("/api/chat")
async def chat(request: ChatRequest):
    if request.model == "ollama":
        chat_result = await ollama_client.chat(
            model="llama2",  # Hardcoded to llama2
            ...
        )
```

**Blog Route Flow:**

```python
# routes/content_generation.py
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "mistral:latest")

async def call_ollama(prompt: str) -> str:
    # Direct HTTP call to Ollama
    response = await client.post(
        f"{OLLAMA_HOST}/api/generate",
        json={"model": OLLAMA_MODEL, "prompt": prompt}
    )
```

### Connection Method

**Option A: Via OllamaClient (Chat)**

```
Chat ‚Üí OllamaClient class ‚Üí HTTP POST to localhost:11434
```

**Option B: Direct HTTP (Blog)**

```
Blog ‚Üí httpx.AsyncClient ‚Üí HTTP POST to localhost:11434
```

**Both end up here:**

```
Ollama API Endpoint: http://localhost:11434/api/generate
or: http://localhost:11434/api/chat
```

---

## üìà Scaling Considerations

### Current Bottleneck: Single Ollama Process

```
Ollama Process (PID: ???)
‚îú‚îÄ Can load 1-3 models in VRAM at once
‚îú‚îÄ Can run 1-2 concurrent requests (default)
‚îú‚îÄ Uses GPU acceleration (RTX 5070)
‚îî‚îÄ Single point of failure
```

### How to Handle High Load

**Option 1: Multiple Ollama Instances (No Parallelism)**

```
‚úÖ More stable
‚ùå Each instance uses VRAM
‚ùå Frontend needs routing logic
‚ùå Complex setup

Example: 2 Ollama instances
- Instance 1: localhost:11434 (llama2)
- Instance 2: localhost:11435 (mistral)
- Chat ‚Üí Instance 1
- Blog ‚Üí Instance 2
```

**Option 2: Task Queue (Recommended for Production)**

```
‚úÖ Professional solution
‚úÖ Single Ollama still works
‚úÖ Better resource management
‚ùå Requires Redis/RabbitMQ

Example: Celery + Redis
- Frontend ‚Üí FastAPI (returns immediately)
- FastAPI ‚Üí Redis (enqueue task)
- Worker ‚Üí Ollama (process sequentially)
- Frontend polls for status (same as now)
```

**Option 3: Ollama Server Configuration**

```
Edit Ollama settings to handle more concurrent requests:
- Set OLLAMA_NUM_PARALLEL=2 (or higher)
- Set OLLAMA_NUM_GPU=1 (or more GPUs)
- Requires restart
```

---

## üîå API Endpoints

### Chat (Synchronous)

```bash
POST /api/chat
{
  "message": "What is AI?",
  "model": "ollama",              # Required: "ollama" for Ollama
  "conversationId": "default",    # For multi-turn tracking
  "temperature": 0.7,
  "max_tokens": 500
}

Response (immediate):
{
  "response": "AI is artificial intelligence...",
  "model": "ollama",
  "conversationId": "default",
  "timestamp": "2025-11-02T06:00:00",
  "tokens_used": 45
}
```

### Blog Generation (Asynchronous)

```bash
# Step 1: Create task
POST /api/content/blog-posts
{
  "topic": "How to use Ollama",
  "style": "technical",
  "tone": "professional",
  "target_length": 1500,
  "tags": ["ai", "ollama"]
}

Response (immediate, < 100ms):
{
  "task_id": "abc-123-def",
  "status": "pending",
  "message": "Blog generation started..."
}

# Step 2: Poll for status
GET /api/content/blog-posts/tasks/{task_id}

Polling response (1st check - still generating):
{
  "task_id": "abc-123-def",
  "status": "processing",
  "created_at": "2025-11-02T06:00:00"
}

Polling response (final - complete):
{
  "task_id": "abc-123-def",
  "status": "completed",
  "created_at": "2025-11-02T06:00:00",
  "result": {
    "title": "How to Use Ollama Locally",
    "slug": "how-to-use-ollama-locally",
    "content": "# How to Use Ollama Locally\n\n...",
    "topic": "How to use Ollama",
    "tags": ["ai", "ollama"],
    "generated_at": "2025-11-02T06:05:00"
  }
}
```

---

## üí° Key Insights

### 1. **Ollama is Shared Resource**

- ‚úÖ Both chat and blog use same Ollama instance
- ‚úÖ They talk to `http://localhost:11434` (same process)
- ‚ö†Ô∏è Can cause contention under high load

### 2. **Design Trade-off**

- Chat: **Sacrifices concurrency** for **immediate response**
- Blog: **Accepts async** to **allow long-running tasks**
- Overall: ‚úÖ Great UX, ‚ö†Ô∏è Limited scalability

### 3. **Current Limitations**

- No concurrency control (requests queue up)
- Single Ollama process (single point of failure)
- No horizontal scaling (can't add more instances easily)
- Task storage in-memory (lost if backend restarts)

### 4. **Perfect for Development**

- ‚úÖ Zero cost (Ollama is free)
- ‚úÖ No API rate limits
- ‚úÖ No internet required
- ‚úÖ Full GPU access (fast on RTX 5070)
- ‚úÖ Great for testing and demos

### 5. **Would Need Enhancement for Production**

- Add task queue (Redis/Celery)
- Add concurrency semaphore
- Add persistent task storage (PostgreSQL)
- Add load balancing across multiple Ollama instances
- Add health checks and auto-restart

---

## üìä Current Flow Diagram

```
FRONTEND (http://localhost:3001)
‚îú‚îÄ Chat Interface
‚îÇ  ‚îî‚îÄ User types message
‚îÇ     ‚îî‚îÄ POST /api/chat
‚îÇ        ‚îî‚îÄ Backend (sync, waits)
‚îÇ           ‚îî‚îÄ Ollama chat (1-2s)
‚îÇ              ‚îî‚îÄ Response appears immediately ‚úÖ
‚îÇ
‚îî‚îÄ Blog Generator
   ‚îî‚îÄ User clicks "Generate"
      ‚îî‚îÄ POST /api/content/blog-posts
         ‚îî‚îÄ Backend (async, returns immediately) ‚úÖ
            ‚îî‚îÄ task_id: "abc-123"
            ‚îî‚îÄ Frontend polls /api/content/blog-posts/tasks/abc-123
               ‚îî‚îÄ Every 2-5 seconds: "Loading..." ‚Üí "Loading..." ‚Üí "Complete!" ‚úÖ
                  ‚îî‚îÄ In background: Ollama generating (2-5 min)

OLLAMA (http://localhost:11434) - Shared by both
‚îú‚îÄ Process 1: Chat request (llama2)
‚îú‚îÄ Process 2: Blog request (mistral) - waits if chat is running
‚îî‚îÄ Sequential execution (one at a time by default)
```

---

## ‚úÖ Summary

| Aspect               | Chat      | Blog          | Notes                                             |
| -------------------- | --------- | ------------- | ------------------------------------------------- |
| **Sync/Async**       | Sync      | Async         | Chat waits for response, Blog returns immediately |
| **Model**            | llama2    | mistral       | Different models for different tasks              |
| **Response Time**    | 1-5s      | 2-10 min      | Chat: quick, Blog: long-running                   |
| **Frontend UX**      | Immediate | Polling       | Chat: instant, Blog: check status                 |
| **Ollama Queue**     | Shares    | Shares        | Single Ollama processes both sequentially         |
| **Scalability**      | Limited   | Limited       | Works for 1-10 concurrent users                   |
| **Production Ready** | ‚úÖ Yes    | ‚ö†Ô∏è Needs work | Add task queue, persistence, concurrency control  |

---

## üöÄ Next Steps (If Needed)

1. **Test High Load:** Send 10 simultaneous blog requests, see what breaks
2. **Add Concurrency Control:** Use `asyncio.Semaphore(max_concurrent=2)`
3. **Add Task Persistence:** Store tasks in PostgreSQL instead of memory
4. **Add Task Queue:** Use Celery + Redis for professional queuing
5. **Add Multiple Ollama:** Run 2-3 Ollama instances for parallel processing

---

**Questions?** Check the code in:

- `src/cofounder_agent/routes/chat_routes.py` - Chat implementation
- `src/cofounder_agent/routes/content_generation.py` - Blog generation
- `src/cofounder_agent/services/ollama_client.py` - Ollama client implementation
