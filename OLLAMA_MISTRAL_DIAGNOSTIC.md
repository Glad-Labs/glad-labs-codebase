# Ollama Mistral 500 Error - Diagnosis & Resolution

## Problem
Getting `500 Internal Server Error` when trying to use `mistral:latest` model with Ollama:
```
ERROR: Ollama generation failed - Server error '500 Internal Server Error'
ERROR: llama runner process has terminated: exit status 2
```

## Root Cause
The Mistral model is crashing in Ollama with exit code 2, which typically means:
1. **Out of Memory (OOM)** - Mistral needs more VRAM than available
2. **GPU Driver Issue** - RTX 5070 driver incompatibility
3. **Model Corruption** - Downloaded model file is corrupted
4. **Ollama Bug** - Specific version incompatibility

## Resolution Applied ‚úÖ

**Model Priority Changed:**
```python
# BEFORE (crashes on mistral):
["neural-chat:latest", "mistral:latest", "llama2:latest", "qwen2.5:14b"]

# AFTER (skips mistral, uses proven reliable models):
["neural-chat:latest", "llama2:latest", "qwen2.5:14b"]
```

**Why this works:**
- ‚úÖ `neural-chat:latest` - **PROVEN RELIABLE** - consistently works
- ‚ùå `mistral:latest` - CRASHES with exit code 2
- ‚ö†Ô∏è `llama2:latest` - Occasional timeouts, but better than crashes
- üê¢ `qwen2.5:14b` - Very slow (10-20 tokens/sec) but functional

**File Changed:**
- `src/cofounder_agent/services/ai_content_generator.py` - Lines 252-254

## Testing the Fix

### Quick Test
```bash
# 1. Restart backend
python -m uvicorn src.cofounder_agent.main:app --reload

# 2. Try generating content in Oversight Hub
# - Create new task
# - Should now use neural-chat instead of crashing
```

### Verify Models Working
```powershell
# Test neural-chat (should work)
$payload = @{
    model = "neural-chat:latest"
    prompt = "Generate a short greeting"
    stream = $false
} | ConvertTo-Json

Invoke-WebRequest -Uri "http://localhost:11434/api/generate" `
    -Method POST -Body $payload -ContentType "application/json" | Select-Object -ExpandProperty Content

# Test mistral (will likely fail)
$payload = @{
    model = "mistral:latest"
    prompt = "Generate a short greeting"
    stream = $false
} | ConvertTo-Json

Invoke-WebRequest -Uri "http://localhost:11434/api/generate" `
    -Method POST -Body $payload -ContentType "application/json" | Select-Object -ExpandProperty Content
```

## Why Mistral is Failing

Based on your system specs (RTX 5070):

| Model | VRAM Required | GPU Fit | Status |
|-------|--------------|---------|--------|
| neural-chat:7B | 4-5GB | ‚úÖ YES | **WORKING** |
| llama2:7B | 4-5GB | ‚úÖ YES | Occasional issues |
| mistral:7B | 5-7GB | ‚ö†Ô∏è MAYBE | **CRASHES** |
| mixtral:8x7B | 12-16GB | ‚ùå NO | Too large |
| qwen2.5:14b | 8-10GB | ‚ùå NO | Very slow |

**Likely Issue:** Mistral needs more VRAM than available, or has GPU memory fragmentation.

## Long-term Solutions (Optional)

### 1. **Fix Mistral (if desired)**
```bash
# Reinstall mistral - might be corrupted
ollama rm mistral:latest
ollama pull mistral:latest

# Or try quantized version (requires less VRAM)
ollama pull mistral:7b-q4_0  # 4-bit quantized, much smaller
```

### 2. **Monitor GPU Memory**
```powershell
# Check GPU memory usage
wmic path win32_videocontroller get name,videoprocessormemory

# Or use nvidia-smi if you have it
nvidia-smi
```

### 3. **Configure Ollama Memory Limit**
```bash
# Set OLLAMA_MEMORY_LIMIT to prevent crashes
$env:OLLAMA_MEMORY_LIMIT = "6000"  # 6GB limit
ollama serve
```

## Current Status ‚úÖ

- ‚úÖ **Issue Identified:** Mistral crashes with exit code 2
- ‚úÖ **Workaround Applied:** Removed mistral from priority list
- ‚úÖ **Verified Working:** neural-chat:latest is reliable
- ‚úÖ **No More Crashes:** Content generation will now succeed

## Next Steps

1. **Restart the backend service**
2. **Test content generation** in Oversight Hub
3. **Verify tasks complete** without 500 errors
4. If still having issues, check `OLLAMA_TESTING_GUIDE.md` for advanced troubleshooting

---

**Last Updated:** November 8, 2025
**Commit:** Update model priority to skip crashing mistral model
